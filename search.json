[{"title":"Ceph MDS InodeTable 分配流程和故障分析","path":"/posts/3495093285/","content":"本文对 Ceph 中 inode 分配流程进行了总结，并解释了 MDS 重启过程中可能遇到的一个故障 inode 分配流程在 Ceph 中我们创建目录、文件、软链接时都需要为其分配一个 inode 作为唯一标识，这一分配过程在 Server::prepare_new_inode 中完成，而分配的依据，也就是哪些 inode 还没有被分配，哪些被分配了还没有生效（落盘）则是记录在 InoTable 中: 123456class InoTable : public MDSTable &#123;...private: interval_set&lt;inodeno_t&gt; free; // unused ids interval_set&lt;inodeno_t&gt; projected_free;... 其中 free 记录的就是没有被分配的 inode 集合，而 projected_free 则是从 free 中剔除了已经被分配但是还没有落盘的部分，因此 projected_free 是 free 的子集，同时显然分配也首先要在 projected_free 中来做。 实际上 interval_set 就是一堆 set 的集合，这里代表可用的 inode 范围的集合，给一个可能的 projected_free 的内容，非常好理解： 1projected_recycle: ..., &#123;begin: 0x100068f03be, len: 49&#125;, &#123;beegin: 0x100068f045b, len: 2&#125;, ... 那么回到 Server::prepare_new_inode， MDS 对于客户端的请求需要返回一个 CInode 结构，首先就需要从 InodeTable 中取出一个可用的 inode，并以此构建 CInode 结构后返回，通常情况下这个过程很简单，我们只需要从 projected_free 中取出可用的第一个 inode 就可以了： 12id = projected_free.range_start();projected_free.erase(id); 这里首先将 inode 从 projected_free 中移除并将其返回给客户端，这个 inode 会在落盘后响应客户端时 apply 并 free 中移除： 1234567891011121314151617void Server::reply_client_request(MDRequestRef&amp; mdr, const ref_t&lt;MClientReply&gt; &amp;reply)&#123;\t...\tapply_allocated_inos(mdr, session);...void Server::apply_allocated_inos(MDRequestRef&amp; mdr, Session *session)&#123;\t...\tmds-&gt;inotable-&gt;apply_alloc_id(mdr-&gt;alloc_ino);...void InoTable::apply_alloc_id(inodeno_t id)&#123;\t...\tfree.erase(id);... 到这里为止 MDS 只有在落盘之后才向客户端返回分配好的 inode，直到一天我们认为在 MDS 刷盘写 Journal 的时候让客户端死等的效率太低了，由此就引入了 early_reply 以使得客户端能够快速接收到 MDS 的处理结果。 首先当我们开启 mds_early_reply 时（实际上默认就是开启的），MDS 会在刷盘之前预先把处理的结果通过 Server::early_reply 返回一个 unsafe reply 给客户端，那么客户端在接收到 MDS 的返回结果后就可以继续进行下一个请求而不必等待 MDS 刷盘，而当 MDS 刷盘完成后则会再给客户端返回一个 safe reply，这时客户端只需要结束该 request 即可。 这时如果在任意处理过程中 MDS 因为一些原因重启，则在客户端可能就会有一部分未接收到 safe_reply 确认的 unsafe_requests 需要 MDS 处理，那么这时客户端就会在 MDS 的 reconnect 阶段重发这些 unsafe_requests 消息以及之前已经发送但还没有收到任何回复的 old_requests， MDS 接收到这些请求之后则会将 unsafe_requests 消息加入 replay_queue 中等到 client_replay 阶段处理，而 old_requests 则会放到 active 阶段再处理。 当 MDS 到达 client_replay 阶段后开始处理客户端 unsafe_requests 请求，这里由于客户端之前收到过 MDS 的 unsafe_reply，因此在发送 unsafe_request 中会携带 MDS 之前的处理结果（也就是之前预分配的 inode 号），而在 MDS 这边对于客户端的 unsafe_request 则的处理会按照重启前已经落盘或还没有落盘分别处理。 如果说一条请求在 MDS 重启前已经落盘，那么在 Server::handle_client_request 中将进入 completed_request 逻辑，首先在 session 的 completed_request 中如果找到 req-&gt;get_reqid().tid，那么就对于这类请求就直接构建一个 MClientReply 并返回给客户端，而不必再执行一遍了： 123456789101112131415161718192021222324void Server::handle_client_request(const cref_t&lt;MClientRequest&gt; &amp;req)&#123;\t...\tbool has_completed = false;\tif (req-&gt;is_replay() || req-&gt;get_retry_attempt()) &#123; inodeno_t created; if (session-&gt;have_completed_request(req-&gt;get_reqid().tid, &amp;created)) &#123; has_completed = true; if (req-&gt;is_replay() || ...) &#123; dout(5) &lt;&lt; &quot;already completed &quot; &lt;&lt; req-&gt;get_reqid() &lt;&lt; dendl; auto reply = make_message&lt;MClientReply&gt;(*req, 0); if (created != inodeno_t()) &#123; bufferlist extra; encode(created, extra); reply-&gt;set_extra_bl(extra); &#125; mds-&gt;send_message_client(reply, session); if (req-&gt;is_queued_for_replay()) mds-&gt;queue_one_replay(); return; &#125;\t...... 那对于没有落盘的 unsafe_request 请求，在 MDS 就没有办法直接完成，而是必须需要 MDS 重新执行一遍请求，那这里就涉及到一个问题，之前我们在 prepare_new_inode 中分配 inode 号只需要拿 projected_free 中的第一个没有被分配的 inode 号即可，那在 MDS 重新执行此请求时我们还是拿第一个 inode 号来分配吗？这样会造成和之前分配的 inode 号不相等的问题吗？ 而实际上为了避免这个问题，客户端在发送 unsafe_request 的时候就会携带之前 MDS 的处理结果，并要求 MDS 再次分配同样的 inode，当然这里能这样做还是基于集群中不存在恶意节点，因此我们相信客户端告知的 inode 确实是 MDS 之前分配出去的： 1234567891011121314151617void Server::handle_client_openc(MDRequestRef&amp; mdr)&#123;\t...\tCInode *newi = prepare_new_inode(mdr, dn-&gt;get_dir(), inodeno_t(req-&gt;head.ino), req-&gt;head.args.open.mode | S_IFREG, &amp;layout);...CInode* Server::prepare_new_inode(MDRequestRef&amp; mdr, CDir *dir, inodeno_t useino, unsigned mode, const file_layout_t *layout)&#123;\t... bool allow_prealloc_inos = mdr-&gt;session-&gt;is_open();\tif (allow_prealloc_inos &amp;&amp; (mdr-&gt;used_prealloc_ino = _inode-&gt;ino = mdr-&gt;session-&gt;take_ino(useino))) &#123; mds-&gt;sessionmap.mark_projected(mdr-&gt;session);\t&#125; else &#123; mdr-&gt;alloc_ino = _inode-&gt;ino = mds-&gt;inotable-&gt;project_alloc_id(useino);\t&#125;... 注意这里 inode 可能是从 session 中取，也有可能从 InodeTable 取，取决于之前 MDS 是否通过 project_alloc_ids 分配给 session 一定数量的 inode，正常情况下对于一个 session 的第一个创建请求， MDS 会在分配 inode 的同时，从 InodeTable 取出一截 inode 号放到 session 中，接着之后的分配请求就都从 session 中分配了，如果 session 中的 inode 快分配完了那么再从 InodeTable 中拿一截再放到 session 中。 MDS 重启故障在上面的过程中无论 MDS 在重启前是否落盘都能 cover 住重启以后客户端发来的 unsafe_requests 消息，这样一来就能够保证 MDS 请求处理流程的幂等性。 但是上面的一切都是建立在 reconnect 以及 client_replay 正常进行的情况下，如果 reconnect 失败， MDS 会如何处理客户端？客户端又会如何处理 unsafe_requests？ 首先我们这里所讨论的 reconnect 失败是由于 MDS 没有在 reconnect 阶段收到客户端发来的 client_reconnect 消息，之前提到客户端会在 MDS 的 reconnect 阶段向 MDS 发送 unsafe_requests 和 old_requests，那实际上客户端还会在所有消息的最后发送一条 client_reconnect 消息标志全部消息都发完了，MDS 在收到这条消息后将客户端从 MDS 的 client_reconnect_gather 中删除，MDS 会在 reconnect 阶段结束后遍历并通过 kill_session 关闭未完成 client_reconnect 的客户端 session。 在 Server::journal_close_session 中 ： 123456789101112131415void Server::journal_close_session(Session *session, int state, Context *on_safe)&#123;\t...\tinterval_set&lt;inodeno_t&gt; inos_to_free; // (1)\tinos_to_free.insert(session-&gt;pending_prealloc_inos);\tinos_to_free.insert(session-&gt;free_prealloc_inos);\t...\tauto fin = new C_MDS_session_finish(this, session, sseq, false, pv, inos_to_free, piv, session-&gt;delegated_inos, mdlog-&gt;get_current_segment(), on_safe); // (2)\t...\twhile(!session-&gt;requests.empty()) &#123; // (3) auto mdr = MDRequestRef(*session-&gt;requests.begin()); mdcache-&gt;request_kill(mdr);\t&#125; 这里 MDS 在 (1) 处回收了之前分配给 session 的 inode, 并在 (3) 处移除了客户端 session 对应的请求，然后等到 (2) 完成后将 session 从 session_map 中移除。 那么一旦客户端没有完成 reconnect， MDS 会在 Server::kill_session 中会一路调到 AsyncConnection 的 shutdown_socket 以关闭 socket 连接，这样一来客户端在 TCP 层面就会感知到对端的 socket 已经被关闭，继而在 Client::_closed_mds_session 中关闭这个 session 对应的请求并关闭 MDS 对应的 session, 这样一来下次 client 再要向 MDS 去发消息的时候就需要通过 Client::_get_or_open_mds_session 重新建立 session。 在 Client::kick_requests_closed 中： 1234567891011121314151617181920212223242526272829void Client::kick_requests_closed(MetaSession *session)&#123;\tfor (map&lt;ceph_tid_t, MetaRequest*&gt;::iterator p = mds_requests.begin(); p != mds_requests.end(); ++p) &#123; ... if (req-&gt;mds == session-&gt;mds_num) &#123; if (req-&gt;caller_cond) &#123; // (1) req-&gt;kick = true; req-&gt;caller_cond-&gt;notify_all(); &#125; req-&gt;item.remove_myself(); // (2) if (req-&gt;got_unsafe) &#123; // (3) req-&gt;unsafe_item.remove_myself(); if (is_dir_operation(req)) &#123; Inode *dir = req-&gt;inode(); dir-&gt;set_async_err(-CEPHFS_EIO); req-&gt;unsafe_dir_item.remove_myself(); &#125; if (req-&gt;target) &#123; InodeRef &amp;in = req-&gt;target; in-&gt;set_async_err(-CEPHFS_EIO); req-&gt;unsafe_target_item.remove_myself(); &#125; signal_cond_list(req-&gt;waitfor_safe); unregister_request(req); &#125; &#125;\t&#125;... 可以看到客户端在 kick_requests_closed 中主要做了三个事情， (1) 处通过 caller_cond-&gt;notify_all() 唤醒所有 wait 在 make_request 处的请求并将其标记为 kick，这些请求都还没有接收到 MDS 的回复，因此会在下一个 while 循环中尝试和 MDS 建立新的连接并重传，接着 (2) 处将请求从 session-&gt;requests 中移除， (3) 中则是针对 unsafe_request 做了额外的处理，首先除了从 session-&gt;requests 中移除外 unsafe_request 还需要从 session-&gt;unsafe_requests 中移除，另外对于目录相关的操作客户端给目录的 inode 设置一个 CEPHFS_EIO (给所有 inode 对应的 fh 设置 async_err)，以及有 MDS 返回 inode 的也给对应的 inode 设置一个 CEPHFS_EIO， 最后将这些不需要重传（因为上层应用已经拿到了返回结果，实际上也无法重传， make_request 已经结束了）的请求通过 unregister_request 将其从 mds_requests 中移除。 那么这里我们做一个简单的对上面部分的总结： 如果客户端 reconnect 失败， MDS 会将客户端 session kill 掉并回收之前分配给客户端的 inode 客户端得知 session 被对端 MDS kill 掉，会将 session 对应的 unsafe_requests 移除，并唤醒所有正在等待回复的请求 被唤醒的请求会在重建 session 后重试 那么我们假设有两个请求 req1 (mkdir dir1 - 创建一个目录)，req2 (touch dir1&#x2F;file1 - 创建该目录下的一个文件)，其中在 MDS 重启前 req1 已经收到了来自 MDS 的 unsafe_reply （但是在 MDS 上对于 req1 的处理还没有落盘）因此客户端继续发送 req2，这时 MDS 重启，客户端 reconnect 超时失败，MDS 于是通过 kill_session 关闭客户端 session，而客户端如上文所说通过 kick_requests_closed 注销 req1 (req1 是 unsafe_requset) 并唤醒 req2 (req2 正在等待 MDS 回复)，那么客户端就会尝试和 MDS 重新建立 session 并重试 req2 这里对于一个 CREATE 请求，客户端需要携带父目录 inode 和要创建的文件的 dentry，因此当请求到达 MDS 端时 MDS 在 MDCache::path_traverse 中就会尝试通过 get_inode 获取父目录，注意这里直接是从 Cache 中获取，因为 MDS 这里认为既然客户端能够发来这个请求那么一定是之前打开过这个目录，那么目录对应的 CInode 就应该在 LRU 中被 pin 住了，当然这里正常流程也有可能出现 Cache 不对等的情况，这涉及到的另一个已经被 fix 的 bug (对于两边 Cache 不对等的情况，MDS 应该尝试从 rados 重建 backtrace）我们这里不做展开，但是无论如何，对于 req1 创建的 inode 对于 MDS 来说确实是不可见的，因此 MDS 将会向客户端返回 ESTALE 错误，而当客户端接收到这个错误之后会继续尝试重传（commit aabd5e9c by Xiubo Li 移除了重传的逻辑）,由此 MDS 和 Client 陷入了无限循环。 除此以外由于客户端在之前收到 unsafe_reply 时已经做了 insert_trace, 目录 dir1 的 inode 已经通过 add_update_inode 加入了 inode_map 中，后续如果有请求分配到了这个 inode （之前提到 MDS 会回收分配给 session 的 inode）的话就有可能造成 inode 结构混乱，例如后续分到这个 inode 的请求是一个 CREATE 请求的话在客户端就会产生混淆（甚至 crash），在客户端重启之前都无法解决。 要解决以上这些问题，最根本的思路就是要解决客户端 _closed_mds_session 时处理一半的问题，要么所有的请求都 ban 掉，要么就让所有请求安全的重传，注意这里的所有也包括 unsafe_requests，因为之后的请求可能和 unsafe_requests 形成依赖关系。这里我选择的思路是后者，因为一来前者已经有人做了（如果不关闭 mds_session_blocklist_on_evict 选项那所有的请求在下一次循环时都会因为 blocklist 取消），再一个后者也更符合 client_reconnect_stale 的语义（毕竟我们开启这个选项就是为了客户端无感知恢复连接） 以上就是对 MDS InoTable 分配流程的总结以及重启过程中 reconnect 失败导致的 unsafe_requests 被丢弃所引发的问题的分析，希望对大家有所帮助。","tags":["Ceph"]},{"title":"Ceph MDS MetaBlob 分析","path":"/posts/3797827770/","content":"CephFS 中会把多种事件记录在不同类型的 LogEvent 中，其中最常见的也是最重要的就是读写流程中处理客户端请求时记录的 EUpdate 事件，而在 EUpdate 中又会把元数据的状态保存在 MetaBlob 结构中，本文就主要对 MetaBlob 结构以及其中保存的内容做一个总结。 MetaBlob 类定义看上去比较复杂，实际最重要的成员只有三个： 1234567class EMetaBlob &#123;public: // my lumps. preserve the order we added them in a list. std::vector&lt;dirfrag_t&gt; lump_order; std::map&lt;dirfrag_t, dirlump&gt; lump_map; std::list&lt;fullbit&gt; roots; 其中 lump_order 保存从根目录到父母路的 Dir 分片信息 dirfrag_t，lump_map 中保存 dirfrag_t 到 dirlump 的映射关系， dirlump 中保存的就是相关的元数据信息，最后 roots 保存的就是 root 节点的 fullbit 信息，fullbit 表示一个存在的 dn + inode，相对的 MetaBlob 中使用 null_bit 结构保存一个不存在的 dentry 直接通过一个例子来理解： 当我们通过 mv 命令将文件 /AppleDir/BananaDir/BananaFile 移动到 /AppleDir/CherryDir/LemonDir/ 同时（重命名）覆盖原来的 MangoFile 文件，在 rename 过程中需要填充的 MetaBlob 信息如下（在 _rename_prepare 中），注意 BananaFile 同时还是 /AppleDir/AppleFile 的硬链接： 给出相关的目录和文件的 inode123456789101112130x601: ~mds0/stray1/0x1: /0x100000102ea: /AppleDir/0x100000102ec: /AppleDir/CherryDir/0x100000102ed: /AppleDir/CherryDir/LemonDir/0x100: ~mds0/0x100000102eb: /AppleDir/BananaDir/0x100000102f4: /AppleDir/CherryDir/LemonDir/MangoFile0x100000102ef: /AppleDir/BananaDir/BananaFile 首先将 stray 目录加入 metablob 1dirlump 0x601 v 631439 state 4 num 0/0/0 通过 predirty_journal_parents oldin destdn-&gt;get_dir 将移动前的 MangoFile 的 parents 加入 metablob 1234567dirlump 0x601 v 631439 state 4 num 0/0/0dirlump 0x1 v 73684 state 4 num 1/0/0 fullbit dn AppleDir [2,head] dnv 73683 inode 0x100000102ea state=dirlump 0x100000102ea v 43 state 0 num 1/0/0 fullbit dn CherryDir [2,head] dnv 42 inode 0x100000102ec state=dirlump 0x100000102ec v 23 state 0 num 1/0/0 fullbit dn LemonDir [2,head] dnv 22 inode 0x100000102ed state= 通过 predirty_journal_parents oldin straydn-&gt;get_dir 将移动后的被删除文件 0x100000102f4 的 parents（也就是 stray 目录）加入 metablob 123456789dirlump 0x601 v 631439 state 4 num 0/0/0dirlump 0x1 v 73684 state 4 num 1/0/0 fullbit dn AppleDir [2,head] dnv 73683 inode 0x100000102ea state=dirlump 0x100000102ea v 43 state 0 num 1/0/0 fullbit dn CherryDir [2,head] dnv 42 inode 0x100000102ec state=dirlump 0x100000102ec v 23 state 0 num 1/0/0 fullbit dn LemonDir [2,head] dnv 22 inode 0x100000102ed state=dirlump 0x100 v 132673 state 4 num 1/0/0 fullbit dn stray1 [2,head] dnv 132672 inode 0x601 state= 通过 predirty_journal_parents srci srcdn-&gt;get_dir() 将移动前的 BananaFile 的 parents 加入 metablob 12345678910dirlump 0x601 v 631439 state 4 num 0/0/0dirlump 0x1 v 73684 state 4 num 1/0/0 fullbit dn AppleDir [2,head] dnv 73683 inode 0x100000102ea state=dirlump 0x100000102ea v 45 state 4 num 2/0/0 fullbit dn CherryDir [2,head] dnv 42 inode 0x100000102ec state= fullbit dn BananaDir [2,head] dnv 44 inode 0x100000102eb state=dirlump 0x100000102ec v 23 state 0 num 1/0/0 fullbit dn LemonDir [2,head] dnv 22 inode 0x100000102ed state=dirlump 0x100 v 132673 state 4 num 1/0/0 fullbit dn stray1 [2,head] dnv 132672 inode 0x601 state= 再通过 predirty_journal_parents srci destdn-&gt;get_dir() 将移动后的新 MangoFile (AppleFile 的硬链接文件) 的 parents 加入 metablob（注意这里和之前的 LemonDir 相比版本 dnv 发生了变化） 1234567891011dirlump 0x601 v 631439 state 4 num 0/0/0dirlump 0x1 v 73684 state 4 num 1/0/0 fullbit dn AppleDir [2,head] dnv 73683 inode 0x100000102ea state=dirlump 0x100000102ea v 45 state 4 num 2/0/0 fullbit dn CherryDir [2,head] dnv 42 inode 0x100000102ec state= fullbit dn BananaDir [2,head] dnv 44 inode 0x100000102eb state=dirlump 0x100000102ec v 25 state 4 num 2/0/0 fullbit dn LemonDir [2,head] dnv 22 inode 0x100000102ed state= fullbit dn LemonDir [2,head] dnv 24 inode 0x100000102ed state=dirlump 0x100 v 132673 state 4 num 1/0/0 fullbit dn stray1 [2,head] dnv 132672 inode 0x601 state= 通过 add_primary_dentry 将 oldin 记录到 stray 目录的 dirlump 中, 表示这个 inode 被删除 123456789101112dirlump 0x601 v 631440 state 4 num 1/0/0 fullbit dn 100000102f4 [2,head] dnv 631439 inode 0x100000102f4 state=dirlump 0x1 v 73684 state 4 num 1/0/0 fullbit dn AppleDir [2,head] dnv 73683 inode 0x100000102ea state=dirlump 0x100000102ea v 45 state 4 num 2/0/0 fullbit dn CherryDir [2,head] dnv 42 inode 0x100000102ec state= fullbit dn BananaDir [2,head] dnv 44 inode 0x100000102eb state=dirlump 0x100000102ec v 25 state 4 num 2/0/0 fullbit dn LemonDir [2,head] dnv 22 inode 0x100000102ed state= fullbit dn LemonDir [2,head] dnv 24 inode 0x100000102ed state=dirlump 0x100 v 132673 state 4 num 1/0/0 fullbit dn stray1 [2,head] dnv 132672 inode 0x601 state= 同样的，通过 add_primary_dentry 将 srci 0x100000102ef 记录到 LemonDir 的 dirlump 中, 这里是表示移动后的 BananaFile，但是由于它同时是 AppleFile 的硬链接所以这里 dump 时候打的是 AppleFile， inode 是一样的 123456789101112131415dirlump 0x601 v 631440 state 4 num 1/0/0 fullbit dn 100000102f4 [2,head] dnv 631439 inode 0x100000102f4 state=dirlump 0x1 v 73684 state 4 num 1/0/0 fullbit dn AppleDir [2,head] dnv 73683 inode 0x100000102ea state=dirlump 0x100000102ea v 45 state 4 num 3/0/0 fullbit dn CherryDir [2,head] dnv 42 inode 0x100000102ec state= fullbit dn BananaDir [2,head] dnv 44 inode 0x100000102eb state= fullbit dn AppleFile [2,head] dnv 41 inode 0x100000102ef state=dirlump 0x100000102ec v 25 state 4 num 2/0/0 fullbit dn LemonDir [2,head] dnv 22 inode 0x100000102ed state= fullbit dn LemonDir [2,head] dnv 24 inode 0x100000102ed state=dirlump 0x100 v 132673 state 4 num 1/0/0 fullbit dn stray1 [2,head] dnv 132672 inode 0x601 state=dirlump 0x100000102ed v 12 state 0 num 0/1/0 remotebit dn MangoFile [2,head] dnv 10 ino 0x100000102ef dirty=1 最后通过 add_null_dentry 向 BananaDir 添加 nullbit 表示删除原来 BananaFile 的 dentry 1234567891011121314151617dirlump 0x601 v 631440 state 4 num 1/0/0 fullbit dn 100000102f4 [2,head] dnv 631439 inode 0x100000102f4 state=dirlump 0x1 v 73684 state 4 num 1/0/0 fullbit dn AppleDir [2,head] dnv 73683 inode 0x100000102ea state=dirlump 0x100000102ea v 45 state 4 num 3/0/0 fullbit dn CherryDir [2,head] dnv 42 inode 0x100000102ec state= fullbit dn BananaDir [2,head] dnv 44 inode 0x100000102eb state= fullbit dn AppleFile [2,head] dnv 41 inode 0x100000102ef state=dirlump 0x100000102ec v 25 state 4 num 2/0/0 fullbit dn LemonDir [2,head] dnv 22 inode 0x100000102ed state= fullbit dn LemonDir [2,head] dnv 24 inode 0x100000102ed state=dirlump 0x100 v 132673 state 4 num 1/0/0 fullbit dn stray1 [2,head] dnv 132672 inode 0x601 state=dirlump 0x100000102ed v 12 state 0 num 0/1/0 remotebit dn MangoFile [2,head] dnv 10 ino 0x100000102ef dirty=1dirlump 0x100000102eb v 11 state 0 num 0/0/1 nullbit dn BananaFile [2,head] dnv 10 dirty=1 流程整理出来之后还是比较容易理解的，可以看到往 MetaBlob 中保存元数据就是通过 predirty_journal_parents、add_primary_dentry 和 add_null_dentry 完成的，而在 MetaBlob 中记录实际上也就是本次操作中涉及到的文件和目录变化。 以上就是对 MetaBlob 结构如何保存元数据信息的总结，希望对大家有所帮助","tags":["Ceph"]},{"title":"Paxos 算法以及 Ceph 实现","path":"/posts/3640854188/","content":"本文包含 Paxos、Multi-Paxos 算法的原理以及 Ceph 中对 Paxos 算法的实现。 Basic Paxos 和 Multi-PaxosPaxos 和 raft 算法看过不少次，但是始终属于一知半解，也没有个记录，这次把 Paxos 和 Multi-Paxos 的原理和 ceph 中 Paxos 的实现都记录一下： 首先是 Paxos 算法，朴素的 Paxos 算法是不包含 leader 角色的： proposer (提议者): 提出提案，包含提案编号（proposal ID）和一个要协商的值（Val） acceptor (决策者)：参与决策，回应收到的来自 proposer 的提案，当多数 acceptor 赞成时提案通过，一般来说一个 proposer 同时也是一个 acceptor Basic Paxos 算法通过两阶段通过一个决议： proposer 向 acceptors 发起 prepare 提案，这时 acceptors 会根据自身情况对提案进行回复，可以选择接受（promise）或者拒绝（可以沉默，也可以告知） proposer 得知多数 acceptor (quorum) 表示接受并承诺了这个提案，那么就会正式向 acceptors 发送 propose 请求，acceptor 会对收到的提案进行 accpet 处理 对于 prepare 消息和 promise 消息的描述如下： prepare 消息不需要携带提案内容，只需要携带全局唯一的 proposal ID 即可（比如时间戳 + rank 号） promise 请求由 acceptor 回复给 proposer，携带自身已经接受的提案中 proposal ID 最大的提案的 ID 和 Val，如果此前没有接受过提案则返回空值 另外如果 acceptor 在 prepare 阶段接受了一个 proposal 提案，那么 acceptor 需要做出两个承诺： 不再接受 proposal ID 小于等于当前请求的 prepare 请求 不再接受 proposal ID 小于当前请求的 propose 请求 proposer 在接收到 quorum 中 acceptor 的回应后，从中选出 proposal ID 最大的提案的 Val 作为本次要发起的提案，如果所有 acceptor 的回复均是空值，那么就可以自己随意决定提案 Valacceptor 收到正式提案处理时接受并持久化当前的 proposal ID 和提案 Val 上述整体过程如下图所示： 注意一旦一个提案被多数 acceptor 接受（即形成决议）之后不会再被推翻，也就是说此后包含其他值的提案都不会再被接受了（即使新的提案拥有更大的 proposal ID） 理解 Basic Paxos 算法，最重要的一点是牢记 Basic Paxos 属于共识算法，它唯一的作用就是保证多个节点对某个值达成共识，更新这个值并不是 Basic Paxos 的任务。 Basic Paxos 具有的问题： 开销大（一次决议需要至少两次网络通信） 可能形成活锁 无法确定多个值 Multi-Paxos 相比于 Basic Paxos 的改进： 针对每个要确定的值执行一次 Paxos，多个 Paxos 实例之间使用 Instance ID 作为区分 引入 leader 的概念，只有 leader 可以提交 proposal，这样以来由于没有 proposer 竞争，一方面解决了活锁问题，另一方面也不需要 prepare，从而将两阶段提交改进为一阶段提交，降低了开销 选举 leader 的过程同样是一次决议的形成（当然也可以通过其他方式）。 如果多个（自认为是） leader 节点提交 proposal 那么将退化成 Basic Paxos，但不会导致不安全（相当于多个 proposer 同时提交 proposal）. 一个可能的 Paxos 运行过程了解了以上内容之后，这里接着给出一个可能的，当前一个 leader 发生故障而新的 leader 被选举出来之后发生的事情（取自 Paxos Made Simple）： 这个新的 leader 也是这个一致性算法的所有实例中的学习者，它应该知道大多数已经被选定的命令。假设它知道 1-134、138 以及 139 号命令，也就是一致性算法的 1-134、138 以及 139 号实例的值。（我们稍后将会看到命令序列中的这样一个空缺是如何产生的。）然后它执行实例135-137以及所有大于139的实例的阶段1，假设这些执行的结果只确定了实例 135 和 140 中提议的值，但是其他实例中没有提议值的约束14。leader 执行实例 135 和 140 的阶段2，并因此可以选定 135 和 140 号命令。 leader 自身就像其他向 leader 学习 leader 所知道的所有命令的别的服务器一样，现在可以运行命令 1-135。因为136号和137号命令还没有选定，所以它还不能运行 138-140 号命令，尽管它知道 138-140 号命令。于是，我们让它通过提议将一个特殊的不会导致状态机状态切换的“no-op”命令作为第136号和137号命令（它可以通过执行一致性算法的 136 号和 137 号实例的阶段 2 来完成），以此快速填补空缺。一旦这些 no-op 命令被选定，那 138-140 号命令就可以被执行了。 现在从 1 到 140 的命令都被选定了。 leader 完成了一致性算法中大于 140 的所有实例的阶段 1，它可以在这些（完成阶段1的）实例的阶段2中自由地提议任意的值。它给某个客户端请求的下一个命令分配了 141 号命令，把它作为这个一致性算法的 141 号实例的阶段2提议的值。它接着将它收到的下一个客户端命令提议为第 142 号命令，以此类推。 leader 可以在它获知它提议的 141 号命令已被选定之前提议 142 号命令。它在提议第 141 号命令中发送的所有消息有可能丢失，而第 142 号命令会在其他服务器获知到 leader 提议的第 141 号命令之前被选定。当 leader 在实例 141 中没有收到对它的阶段2的预期响应时，它将会重发这些消息。如果一切顺利，它提议的命令会被选定。无论如何，它还是有可能在前面有失败，在选定的命令的序列上留下一段空缺。一般来说，假设一个 leader 可以提前获得 α 个命令——也就意味着，它可以在 1 到 i 号命令被选定之后提议第 i + 1 到 i + α 号命令。一个多达 α - 1 个命令的空缺可能随之形成。 一个新的被选定的 leader 执行一致性算法中的无限多的实例的阶段1——如果是在上面的场景中，就是实例 135-137，以及所有大于139的实例。让所有实例使用一样的提案编号，它可以通过向其他的服务器发送一个合理的短消息来实现这一点。在阶段1中，接受者当且仅当它已经收到了某个提议者的阶段2的消息的时候，它才会响应不止1个简单的OK。（在这个场景里，这是仅针对实例 135 和 140 的例子。）所以，一个（扮演接受者的）服务器可以用一个单一且合理短的消息回应所有的实例。因此，执行阶段1的无穷多个实例不会带来任何问题。 由于 leader 的故障以及新 leader 的选举理应很少发生，因此执行状态机命令——对命令&#x2F;值达成一致的过程的有效成本，仅为运行这个一致性算法的阶段2的成本。可以看出，Paxos 一致性算法的第2阶段在存在故障的情况下，其达成协议的可能代价是所有算法中最小的。于是，Paxos 算法本质上是最优的。 Ceph 中的 Paxos 算法实现通过上述章节我们基本了解了 Basic Paxos 和 Multi-Paxos 的基本运行方式，这一部分就来看一下 Ceph 中对于 Paxos 算法的实现和使用方式（Multi-Paxos 并没有具体的实现细节，所以 Ceph 中的只是其中一种实现方式）。 首先 Ceph 中的 Paxos (指 Multi-Paxos，下同) 在 Monitor 中实现，用于维护 monmap、osdmap、mdsmap、pgmap 等各种 map 在 Mon 启动时会初始化 Paxos（在 Monitor::init_paxos 中）： 123456789101112131415void Paxos::init()&#123; // load paxos variables from stable storage last_pn = get_store()-&gt;get(get_name(), &quot;last_pn&quot;); accepted_pn = get_store()-&gt;get(get_name(), &quot;accepted_pn&quot;); last_committed = get_store()-&gt;get(get_name(), &quot;last_committed&quot;); first_committed = get_store()-&gt;get(get_name(), &quot;first_committed&quot;); dout(10) &lt;&lt; __func__ &lt;&lt; &quot; last_pn: &quot; &lt;&lt; last_pn &lt;&lt; &quot; accepted_pn: &quot; &lt;&lt; accepted_pn &lt;&lt; &quot; last_committed: &quot; &lt;&lt; last_committed &lt;&lt; &quot; first_committed: &quot; &lt;&lt; first_committed &lt;&lt; dendl; dout(10) &lt;&lt; &quot;init&quot; &lt;&lt; dendl; ceph_assert(is_consistent());&#125; 这里从存储中获取已经被持久化的 last-pn、accpeted_pn、last_committed 和 first_committed， 其中 pn 就是 proposal num 也就是提案编号， committed 则是记录已提交的版本也就是已经达成决议的 Paxos 实例号 Probe 阶段接着 Mon 会将自己的状态置为 STATE_PROBING，开始 bootstrap 以发现集群并同步数据: 12state = STATE_PROBING;bootstrap(); 首先将自己加入一个外部的多数派集合（也就是首先默认自己不在当前集群 quorum，因为还没有同步数据）: 123// i&#x27;m outside the quorumif (monmap-&gt;contains(name)) outside_quorum.insert(name); 接着向其他所有已知的 Mon 发送 MMonProbe 消息，其他 Mon 接到消息后进入 Monitor::handle_probe_probe 处理，这里有两种情况，正常情况下收到请求的 Mon 会向请求方返回应答，包括 leadr、 quorum 成员列表以及当前节点的 first_committed 和 last_committed，另一种情况则是发现请求方的 paxos 状态反而远新于自身，那么就会主动使当前节点 re-boostrap 重新同步。 123456789101112131415if (!is_probing() &amp;&amp; !is_synchronizing()) &#123; // If the probing mon is way ahead of us, we need to re-bootstrap. // Normally we capture this case when we initially bootstrap, but // it is possible we pass those checks (we overlap with // quorum-to-be) but fail to join a quorum before it moves past // us. We need to be kicked back to bootstrap so we can // synchonize, not keep calling elections. if (paxos-&gt;get_version() + 1 &lt; m-&gt;paxos_first_version) &#123; dout(1) &lt;&lt; &quot; peer &quot; &lt;&lt; m-&gt;get_source_addr() &lt;&lt; &quot; has first_committed &quot; &lt;&lt; &quot;ahead of us, re-bootstrapping&quot; &lt;&lt; dendl; bootstrap(); goto out; &#125;&#125; 接着回到刚启动的 Mon 通过 Monitor::handle_probe_reply 处理其他 Mon 的回复，同样会对其他 Mon 的 committed version 进行检查，如果中间有空隙或者需要版本差别太远那么就需要从其他 Mon 全量复制： 1234567891011121314151617181920 if (paxos-&gt;get_version() &lt; m-&gt;paxos_first_version &amp;&amp;m-&gt;paxos_first_version &gt; 1) &#123; // no need to sync if we&#x27;re 0 and they start at 1. dout(10) &lt;&lt; &quot; peer paxos first versions [&quot; &lt;&lt; m-&gt;paxos_first_version &lt;&lt; &quot;,&quot; &lt;&lt; m-&gt;paxos_last_version &lt;&lt; &quot;]&quot; &lt;&lt; &quot; vs my version &quot; &lt;&lt; paxos-&gt;get_version() &lt;&lt; &quot; (too far ahead)&quot; &lt;&lt; dendl; cancel_probe_timeout(); sync_start(other, true); return; &#125; if (paxos-&gt;get_version() + g_conf()-&gt;paxos_max_join_drift &lt; m-&gt;paxos_last_version) &#123; dout(10) &lt;&lt; &quot; peer paxos last version &quot; &lt;&lt; m-&gt;paxos_last_version &lt;&lt; &quot; vs my version &quot; &lt;&lt; paxos-&gt;get_version() &lt;&lt; &quot; (too far ahead)&quot; &lt;&lt; dendl; cancel_probe_timeout(); sync_start(other, false); return; &#125; 除此以外暂时先不用立刻同步到最新版本，此时如果对方处于 quorum 中并且自身在 monmap 中那么就可以通过 start_election 加入 quorum: 1234567891011121314if (m-&gt;quorum.size()) &#123; bool in_map = false; const auto my_info = monmap-&gt;mon_info.find(name); const map&lt;string,string&gt; *map_crush_loc&#123;nullptr&#125;; if (my_info != monmap-&gt;mon_info.end()) &#123; in_map = true; map_crush_loc = &amp;my_info-&gt;second.crush_loc; &#125; if (in_map &amp;&amp; !monmap-&gt;get_addrs(name).front().is_blank_ip() &amp;&amp; (!need_set_crush_loc || (*map_crush_loc == crush_loc))) &#123; // i&#x27;m part of the cluster; just initiate a new election start_election(); &#125; 或者发现不在 quorum 中的 mon 数量已经超过了半数，那么就可以通过选举来形成一个新的多数派： 12345678910111213if (monmap-&gt;contains(m-&gt;name)) &#123; dout(10) &lt;&lt; &quot; mon.&quot; &lt;&lt; m-&gt;name &lt;&lt; &quot; is outside the quorum&quot; &lt;&lt; dendl; outside_quorum.insert(m-&gt;name);&#125;unsigned need = monmap-&gt;min_quorum_size();dout(10) &lt;&lt; &quot; outside_quorum now &quot; &lt;&lt; outside_quorum &lt;&lt; &quot;, need &quot; &lt;&lt; need &lt;&lt; dendl;if (outside_quorum.size() &gt;= need) &#123; if (outside_quorum.count(name)) &#123; dout(10) &lt;&lt; &quot; that&#x27;s enough to form a new quorum, calling election&quot; &lt;&lt; dendl; start_election(); &#125;&#125; Election 选举选举阶段通过 Monitor::start_election 开始： 1234567void Monitor::start_election()&#123; dout(10) &lt;&lt; &quot;start_election&quot; &lt;&lt; dendl; state = STATE_ELECTING; ... elector.call_election();&#125; 进入到 ElectionLogic::start 中，Mon 通过 epoch 的奇偶性判断当前 Mon 是否处于选举状态，如果 epoch 是偶数即稳定态，则需要使 epoch 加一进入选举态： 12if (epoch % 2 == 0) &#123; bump_epoch(epoch+1); // odd == election cycle 接着在 Elector::propose_to_peers 中向 monmap 中的其他 Mon 发送 MMonElection::OP_PROPOSE 消息： 123456789101112131415void Elector::propose_to_peers(epoch_t e, bufferlist&amp; logic_bl)&#123; // bcast to everyone else for (unsigned i=0; i&lt;mon-&gt;monmap-&gt;size(); ++i) &#123; if ((int)i == mon-&gt;rank) continue; MMonElection *m = new MMonElection(MMonElection::OP_PROPOSE, e, peer_tracker.get_encoded_bl(), logic.strategy, mon-&gt;monmap); m-&gt;sharing_bl = logic_bl; m-&gt;mon_features = ceph::features::mon::get_supported(); m-&gt;mon_release = ceph_release(); mon-&gt;send_mon_message(m, i); &#125; &#125; 那么其他 Mon 在收到 MMonElection::OP_PROPOSE 后进入 Elector::handle_propose 处理，这里如果不满足一些条件的话会向请求客户端发一个 MMonElection::OP_NAK 消息，接着进入 ElectionLogic::receive_propose 中，这里对于不同的 strategy 设置进入不同的处理流程，在 CLASSIC 也就是默认策略下 Ceph 会人为制造 Mon 直接地位的不平等，也就是 rank 号越小的 Mon 越能赢得选举，那么如果对方的 rank 号比自己小的话就会通过 defer 赞成对方，同样的如果对方 rank 比自己还大，但自己又没有在选举状态的话那么就会发起一个新的选举尝试： 12345678910111213141516171819202122if (elector-&gt;get_my_rank() &lt; from) &#123; // i would win over them. if (leader_acked &gt;= 0) &#123; // we already acked someone ceph_assert(leader_acked &lt; from); // and they still win, of course ldout(cct, 5) &lt;&lt; &quot;no, we already acked &quot; &lt;&lt; leader_acked &lt;&lt; dendl; &#125; else &#123; // wait, i should win! if (!electing_me) &#123; elector-&gt;trigger_new_election(); &#125; &#125;&#125; else &#123; // they would win over me if (leader_acked &lt; 0 || // haven&#x27;t acked anyone yet, or leader_acked &gt; from || // they would win over who you did ack, or leader_acked == from) &#123; // this is the guy we&#x27;re already deferring to defer(from); &#125; else &#123; // ignore them! ldout(cct, 5) &lt;&lt; &quot;no, we already acked &quot; &lt;&lt; leader_acked &lt;&lt; dendl; &#125;&#125; 在 ElectionLogic::defer 中会记录自己已经响应过的 Mon，如果新的 Propose 请求的 rank 小于自己曾经回应过的 Mon 才会回应，否则的话直接忽略掉。 当 Mon 收到 MMonElection::OP_ACK 回复后进入 Elector::handle_ack 处理，首先将对方加入 acked_me 中，如果所有人都承认自己的话宣布胜利，也就是 Paxos 的第二阶段，向其他 Mon 发送 MMonElection::OP_VICTORY 消息： 1234567891011121314151617181920void ElectionLogic::receive_ack(int from, epoch_t from_epoch)&#123; ... acked_me.insert(from); if (acked_me.size() == elector-&gt;paxos_size()) &#123; // if yes, shortcut to election finish declare_victory(); &#125;...void Elector::message_victory(const std::set&lt;int&gt;&amp; quorum)&#123; ... // tell everyone! for (set&lt;int&gt;::iterator p = quorum.begin(); p != quorum.end(); ++p) &#123; if (*p == mon-&gt;rank) continue; MMonElection *m = new MMonElection(MMonElection::OP_VICTORY, get_epoch(), peer_tracker.get_encoded_bl(), logic.strategy, mon-&gt;monmap);... 那如果在一定时间内没有收到所有人的回复的话，那么按照 Paxos 的规则只要大多数也就是过半的 Mon 表示同意的话也能形成 quorum，此时同样可以宣布选举胜利： 1234567891011void ElectionLogic::end_election_period()&#123; ldout(cct, 5) &lt;&lt; &quot;election period ended&quot; &lt;&lt; dendl; // did i win? if (electing_me &amp;&amp; acked_me.size() &gt; (elector-&gt;paxos_size() / 2)) &#123; // i win declare_victory(); &#125; else &#123;... 那么宣布胜利之后 Mon 就会把自己的状态设为 STATE_LEADER (这个过程在 Monitor::win_election 中完成)，其他成员则把自己设为 STATE_PEON Recovery 阶段之前提到选举之前如果自己记录的 Propose 有部分缺失的话是不需要立刻恢复的，那么这里成为 leader 之后就要开始 Paxos Recovery 了，也就是之前示例中填补空洞的过程，在 Monitor::win_election 中调用 Paxos::leader_init 开始： 12345678910111213141516171819202122232425262728293031void Monitor::win_election(epoch_t epoch, const set&lt;int&gt;&amp; active, uint64_t features, const mon_feature_t&amp; mon_features, ceph_release_t min_mon_release, const map&lt;int,Metadata&gt;&amp; metadata)&#123; ... paxos-&gt;leader_init();...void Paxos::leader_init()&#123; cancel_events(); new_value.clear(); // discard pending transaction pending_proposal.reset(); reset_pending_committing_finishers(); logger-&gt;inc(l_paxos_start_leader); if (mon.get_quorum().size() == 1) &#123; state = STATE_ACTIVE; return; &#125; state = STATE_RECOVERING; lease_expire = &#123;&#125;; dout(10) &lt;&lt; &quot;leader_init -- starting paxos recovery&quot; &lt;&lt; dendl; collect(0);&#125; 未完待续 Propose 提案未完待续 References 《Paxos Made Simple》中文翻译：Paxos 如此简单 Paxos算法详解 Implementing Replicated Logs with Paxos Paxos 结合ceph实现 ceph-mon的Leader Elect机制 ceph monitor paxos的实现(一)","tags":["Ceph"]},{"title":"Ceph 读写流程：客户端写流程分析","path":"/posts/748254147/","content":"本文对 Ceph 客户端写流程进行了梳理和总结，包括 ll_open 和 ll_write 的处理流程。 正文首先用户需要通过 ll_open 打开文件并拿到 fh，第一步先做权限检查，进入 may_open, 这里会根据用户带回的 flags 确定 want 是 MAY_WRITE 还是 MAY_READ, 接着通过一次 getattr 拿到文件的 mode, 当然这里 getattr 不是每次都发，只有 client 认为自己的所持有的文件权限不是最新的时候才会发，具体有两种情况： 开启 acl 并且没有拿过文件的 xattr (acl 设置是外带在 xattr 中的，不在 inode 里) caps_issued_mask 认为当前 client 的 caps 不足以保证所持有的权限为最新时 Client::_getattr 判断是否需要发送请求123456789101112131415161718int Client::_getattr(Inode *in, int mask, const UserPerm&amp; perms, bool force)&#123; bool yes = in-&gt;caps_issued_mask(mask, true); if (yes &amp;&amp; !force) return 0;...// force 和 mask:int Client::_getattr_for_perm(Inode *in, const UserPerm&amp; perms)&#123; int mask = CEPH_STAT_CAP_MODE; bool force = false; if (acl_type != NO_ACL) &#123; mask |= CEPH_STAT_CAP_XATTR; force = in-&gt;xattr_version == 0; &#125; return _getattr(in, mask, perms, force);&#125; 简单解释一下第二点就是如果 client 没有持有 As （开启 acl 的话还需要 Xs）那么就需要向 mds 发 getattr拿到 perm 之后做一个 inode_permission 检查一下 mode (如果设了 acl 那就检查 acl) 接着进到 _open, 先把 flags (O_RDONLY, ORDWR, O_CREAT…) 转成 ceph 内部的 cmode (CEPH_FILE_MODE_PIN, CEPH_FILE_MODE_RD, CEPH_FILE_MODE_RDWR, …) 和对应的 want_cap (CEPH_CAP_PIN p, CEPH_CAP_FILE_SHARED Fs, CEPH_CAP_FILE_RD Fr, …) 如果 client 的 caps 满足就直接把 inode 的 ref +1 就可以返回了，否则需要给 mds 发一个 open 消息 Client::_open 判断是否发送 CEPH_MDS_OP_OPEN123456if ((flags &amp; O_TRUNC) == 0 &amp;&amp; in-&gt;caps_issued_mask(want)) &#123; // update wanted? check_caps(in, CHECK_CAPS_NODELAY);&#125; else &#123; MetaRequest *req = new MetaRequest(CEPH_MDS_OP_OPEN); ... mds 消息回来之后通过 _create_fh 创建了一个 Fh 加入到 inode 的 fhs 并返回给用户, 同时将这个 fh 加入 client 的 ll_unclosed_fh_set 中 接着用户拿到 Fh 之后就可以调用 ll_write 并传入刚刚拿到的 Fh，偏移 off, 写入长度 len 和数据 data首先第一步 _write 先做一些基本检查 (判断一下 offset+len 范围是否合法，pool 有没有 full) ，接着做一下可写性检查(Fh 是不是以可写模式打开的 CEPH_FILE_MODE_WR，会不会超 quota)另外写之前还需要把 buf 或者 iov 中的数据写到 bl 中 (也就是同时包含 write 和 writev 的功能),前期的准备就算完了 将 need 和 want 的 Caps 传入 Client::get_caps (need 是必须的 Fw 和 As， want 是可选的 Fb 或者 Fl)Client::get_caps 内的过程比较复杂: 首先通过 Fh 的 open mode 获取 file_wanted (比如以 RD 方式打开的就需要 Fs Fr Fc，和之前 open 时是一样的)接着通过 Inode::caps_issued 获取当前 inode 所拥有的 caps如果当前已经拥有 Fw 那么检查一下 max_size 够不够写 wanted_max_size，如果不够就需要通过 check_caps 去向 mds 申请 如果说此时拥有 need 并且没有 want 的 caps 被 revoking 那么直接 get_cap_ref(need) 然后返回，否则就需要 waitfor_caps caps 问题解决之后就可以开始写了，如果是 O_DIRECT 写的话要把 buffer 和 lazyio 去掉 接着如果是 Direct 写那就直接先 flush_range 然后通过 filer-&gt;write_trunc 去写, 否则的话先通过 objectcacher-&gt;file_write 写到缓存里再 flush Client::_write123456789101112131415161718192021222324252627// 非 Direct 写 if (cct-&gt;_conf-&gt;client_oc &amp;&amp; (have &amp; (CEPH_CAP_FILE_BUFFER | CEPH_CAP_FILE_LAZYIO))) &#123; ... // async, caching, non-blocking. r = objectcacher-&gt;file_write(&amp;in-&gt;oset, &amp;in-&gt;layout, in-&gt;snaprealm-&gt;get_snap_context(), offset, size, bl, ceph::real_clock::now(), 0); ... // flush cached write if O_SYNC is set on file fh // O_DSYNC == O_SYNC on linux &lt; 2.6.33 // O_SYNC = __O_SYNC | O_DSYNC on linux &gt;= 2.6.33 if ((f-&gt;flags &amp; O_SYNC) || (f-&gt;flags &amp; O_DSYNC)) &#123; _flush_range(in, offset, size); &#125;// Direct 写 &#125; else &#123; if (f-&gt;flags &amp; O_DIRECT) _flush_range(in, offset, size); ... filer-&gt;write_trunc(in-&gt;ino, &amp;in-&gt;layout, in-&gt;snaprealm-&gt;get_snap_context(), offset, size, bl, ceph::real_clock::now(), 0, in-&gt;truncate_size, in-&gt;truncate_seq, &amp;onfinish); &#125; 再说一下 filer-&gt;write_trunc 写的过程： 首先通过 Striper::file_to_extents 通过 ino 找到实际要写的 objects (对原始请求的拆分)， Striper::file_to_extents 的流程见 Ceph 读写流程：file_to_extents 过程分析 接着就是通过 objecter-&gt;sg_write_trunc 提交 op 到 OSD 完成写入 写入完成后进入到 success 完成一些收尾工作即可，包括更新文件的 size、mtime，标记 CEPH_CAP_FILE_WR 为 dirty（这样其他客户端要写的话 MDS revoke caps 之前就可以告诉 MDS 这个 Inode 需要同步）以及通过 put_cap_ref 释放刚刚获取的 Fwb 权限等。 以上就是对 Ceph 客户端中写流程的分析和总结,希望能对大家有所帮助！","tags":["Ceph"]},{"title":"Ceph 读写流程：file_to_extents 过程分析","path":"/posts/2816966616/","content":"本文介绍 Ceph 中条带化的相关概念并对 Striper::file_to_extents 的流程进行了分析和总结。 在 Client::_write 中客户端通过 filer-&gt;write_trunc 对文件进行写入，write_trunc 其中分为两步: 通过 Striper::file_to_extents 对请求进行拆分，也就是将对文件的读写请求转换为对对象的读写请求 通过 objecter-&gt;sg_write_trunc 把数据写到 object 中 本文主要对其中的第一步，也就是 Striper::file_to_extents 的过程进行分析。 Ceph 中的 Stripping在正式开始先简单介绍一下 Ceph 中条带化（Stripping）读写的相关概念： 为什么要使用 Stripping最主要的原因就是由于单个存储设备存在吞吐量限制，例如一块机械硬盘的写入能力最大也不过 200MB&#x2F;s, 在与内存、固态缓存甚至是万兆网络的吞吐量对比下无疑就是整个系统中最大的短板，那么条带化就是用来解决存储设备性能瓶颈，提高存储系统吞吐量的东西。那么什么是条带化，说的准确一点就是把信息的连续部分存储在多个存储设备上，说人话就是把一个文件切成多份放在不同的磁盘上，每次对文件中一段内容的读写会同时发给多个盘，这样的话我们实际的写入速度就变成了多个磁盘的写入速度总和。 Ceph 中的条带化的概念大致和其他存储没有区别，只不过在 Ceph 中所有的存储最后都会落到 object 也就是对象上，而对象的存储是没有条带化的，因此文件的条带化实际上是在客户端上进行的，也就是我们本文的主要内容 file_to_extents 的过程 以下图为例，对文件数据的写入首先会在 ObjectSet1 的 Object0 的 stripe unit0 中进行, 当第一个条带被写满之后，后续的写会移动到 Object1 的 stripe unit1 上进行，直到 Object3 的 stripe unit3 被写满之后 Ceph 会根据 ObjectSet 中可容纳的对象数量判断是否写满，并将后续的写转回到 Object0 上的 stripe unit4 中进行。而这个过程在一次包含比较多数据的写中当然是可以并行执行的，只要我们提前计算好写入涉及的 objects 即可。 file_to_extents 流程概念理解以后我们就正式来看 file_to_extents 的过程，首先看一下在 Filer::write_trunc 是如何调用 file_to_extents 的: 123// osdc/Filer.cc// void write_trunc Striper::file_to_extents(cct, ino, layout, offset, len, truncate_size, extents); 这里传入的 ino offset 和 len 比较好理解，就是我们要写入的文件的 Inode 号，然后从哪里开始写，要写多长，注意这里因为只是算映射所以是不需要传 bl 也就是真正要写入的数据的。 接着 truncate_size 和 truncate 写有关所以我们先不过多关心，那么 layout 就是用来辅助我们切分请求的，而 extents 就是我们稍后真正要写入的 ObjectExtent 集合了，定义如下： 1std::vector&lt;ObjectExtent&gt; extents; ObjectExtent 就是我们对于某一个 object 要写入的条带的集合，定义如下： 1234567891011121314151617class ObjectExtent&#123;public: object_t oid; // object id uint64_t objectno; uint64_t offset; // in object uint64_t length; // in object uint64_t truncate_size; // in object object_locator_t oloc; // object locator (pool etc) std::vector&lt;std::pair&lt;uint64_t,uint64_t&gt; &gt; buffer_extents; // off -&gt; len. extents in buffer being mapped (may be fragmented bc of striping!) ObjectExtent() : objectno(0), offset(0), length(0), truncate_size(0) &#123;&#125; ObjectExtent(object_t o, uint64_t ono, uint64_t off, uint64_t l, uint64_t ts) : oid(o), objectno(ono), offset(off), length(l), truncate_size(ts) &#123; &#125;&#125;; 其中我们通过 oid 和 objectno 找到一个 object，而 oid 就是文件的 inode 号。 offset 和 length 则是要写入 object 的这段数据在 object 中的偏移和长度（从上图我们可以看到一段连续的数据按条带写到 object 中实际也是连续的） buffer_extents 则是要写入 object 的数据在 buffer 中的偏移和长度，这里的 buffer 指的是实际的数据，比如一个刚好写满上图 5 个条带的数据必然会横跨三个条带，这时我们就需要在 buffer_extents 中记录下 buffer 中要写入 object 的两段数据的偏移和各自的长度。 理解了上述部分以后，我们就实际看一下 file_to_extents 是如何填充 extents 的： 1234567891011static void file_to_extents(CephContext *cct, inodeno_t ino, const file_layout_t *layout, uint64_t offset, uint64_t len, uint64_t trunc_size, std::vector&lt;ObjectExtent&gt;&amp; extents) &#123; // generate prefix/format char buf[32]; snprintf(buf, sizeof(buf), &quot;%llx.%%08llx&quot;, (long long unsigned)ino); file_to_extents(cct, buf, layout, offset, len, trunc_size, extents);&#125; 第一步就是将 ino 写入 buf 然后进入第二个版本的重载： 1234567891011void Striper::file_to_extents(CephContext *cct, const char *object_format, const file_layout_t *layout, uint64_t offset, uint64_t len, uint64_t trunc_size, std::vector&lt;ObjectExtent&gt;&amp; extents, uint64_t buffer_offset)&#123; striper::LightweightObjectExtents lightweight_object_extents; file_to_extents(cct, layout, offset, len, trunc_size, buffer_offset, &amp;lightweight_object_extents);... 这里可以看到又多了一个 LightweightObjectExtents，实际上这个和 ObjectExtent 的区别就是没有 object_locator_t oloc，进入第三个版本的重载： 12345void Striper::file_to_extents( CephContext *cct, const file_layout_t *layout, uint64_t offset, uint64_t len, uint64_t trunc_size, uint64_t buffer_offset, striper::LightweightObjectExtents* object_extents) &#123;... 首先读一下 layout，object_size 和 stripe_unit 我们刚刚都提到了，stripe_count 实际就是每个 ObjectSet 中包含的 object 数量： 1234// void Striper::file_to_extents__u32 object_size = layout-&gt;object_size;__u32 su = layout-&gt;stripe_unit;__u32 stripe_count = layout-&gt;stripe_count; 如果 stripe_count 为 1 的话切条带就没有意义了，因此我们直接让一个 object 中只包含一个条带即可： 1234if (stripe_count == 1) &#123; ldout(cct, 20) &lt;&lt; &quot; sc is one, reset su to os&quot; &lt;&lt; dendl; su = object_size;&#125; 接下来就进入到切分的过程，这个过程稍微有点长，其中涉及的变量如下： Striper::file_to_extents1234567891011121314151617181920212223uint64_t stripes_per_object = object_size / su;uint64_t cur = offset;uint64_t left = len;while (left &gt; 0) &#123; // layout into objects uint64_t blockno = cur / su; // which block // which horizontal stripe (Y) uint64_t stripeno = blockno / stripe_count; // which object in the object set (X) uint64_t stripepos = blockno % stripe_count; // which object set uint64_t objectsetno = stripeno / stripes_per_object; // object id uint64_t objectno = objectsetno * stripe_count + stripepos; // map range into object uint64_t block_start = (stripeno % stripes_per_object) * su; uint64_t block_off = cur % su; uint64_t max = su - block_off; uint64_t x_offset = block_start + block_off; uint64_t x_len; 插入 object_extents 的过程分两种情况，其中 if 分支是插入一个新 LightweightObjectExtent，else 分支则是写满一组条带之后更新 LightweightObjectExtent 的过程 首先看 if 分支，满足下列条件其中之一的进入此分支： object_extents 为空 当前 object_extent 不存在于 object_extents 中 写入的数据在 object 中不是连续的 进入分支以后直接插入一个 LightweightObjectExtent 到 object_extents 即可： 123456789101112striper::LightweightObjectExtent* ex = nullptr;auto it = std::upper_bound(object_extents-&gt;begin(), object_extents-&gt;end(), objectno, OrderByObject());striper::LightweightObjectExtents::reverse_iterator rev_it(it);if (rev_it == object_extents-&gt;rend() || rev_it-&gt;object_no != objectno || rev_it-&gt;offset + rev_it-&gt;length != x_offset) &#123; // expect up to &quot;stripe-width - 1&quot; vector shifts in the worst-case ex = &amp;(*object_extents-&gt;emplace( it, objectno, x_offset, x_len, object_truncate_size(cct, layout, objectno, trunc_size))); ldout(cct, 20) &lt;&lt; &quot; added new &quot; &lt;&lt; *ex &lt;&lt; dendl; 接着 else 分支则是对 LightweightObjectExtent 中的 length 进行更新： 1234567&#125; else &#123; ex = &amp;(*rev_it); ceph_assert(ex-&gt;offset + ex-&gt;length == x_offset); ldout(cct, 20) &lt;&lt; &quot; adding in to &quot; &lt;&lt; *ex &lt;&lt; dendl; ex-&gt;length += x_len;&#125; 以上两种情况我们都需要对 buffer_extents 进行更新，插入这段数据在 buffer 中的偏移和长度，并更新下一轮我们待切分的数据长度 left 和已经切分的偏移位置 cur： 123ex-&gt;buffer_extents.emplace_back(cur - offset + buffer_offset, x_len);left -= x_len;cur += x_len; 全部切分完成并加入 object_extents 之后我们回到第二个版本的 Striper::file_to_extents 中，在这里遍历 lightweight_object_extents 并填入 extents，这里基本上就是原样填充，只多了一步就是算了一下 oloc： 将 LightweightObjectExtent 转换为 ObjectExtent1234567891011121314151617// convert lightweight object extents to heavyweight versionextents.reserve(lightweight_object_extents.size());for (auto&amp; lightweight_object_extent : lightweight_object_extents) &#123; auto&amp; object_extent = extents.emplace_back( object_t(format_oid(object_format, lightweight_object_extent.object_no)), lightweight_object_extent.object_no, lightweight_object_extent.offset, lightweight_object_extent.length, lightweight_object_extent.truncate_size); object_extent.oloc = OSDMap::file_to_object_locator(*layout); object_extent.buffer_extents.reserve( lightweight_object_extent.buffer_extents.size()); object_extent.buffer_extents.insert( object_extent.buffer_extents.end(), lightweight_object_extent.buffer_extents.begin(), lightweight_object_extent.buffer_extents.end());&#125; 以上就是关于 Ceph 条带化的介绍和 Striper::file_to_extents 的全部过程。 References Ceph Architecture Ceph 客户端","tags":["Ceph"]},{"title":"Ceph 挂载点压缩提示 File changed as we read it 问题分析","path":"/posts/3535257457/","content":"使用 CephFS 的同学可能会发现如果在 CephFS 挂载点下去压缩文件的时候有时候会提示 File changed as we read it，有时候还会导致CRC 失败，本文对 File changed as we read it 问题出现的原因进行了分析并给出了解决办法，希望能对大家有所帮助。 问题现象首先这个问题可能导致的现象比较多样，最基础的就是把一个压缩文件解压一下然后立马压缩，这时就会提示 File changed as we read it 如下： 1234[root@An1Uit ceph-fuse]# ./tar.shtar: cmake-3.7.2/Tests/LinkFlags: 在我们读入文件时文件发生了变化tar: cmake-3.7.2/Tests/SimpleCOnly: 在我们读入文件时文件发生了变化tar: cmake-3.7.2/Tests/ObjC++: 在我们读入文件时文件发生了变化 那除了这个现象以外还可能导致一些应用的 CRC 校验失败，或者 nfs 的 REMOTE I&#x2F;O ERROR 原因分析虽然分析问题时候比较令人挠头但是实际问题的原因比较简单，在 client 配置中有一个默认开启的配置项 client_dirsize_rbytes，开启这个选项之后会影响 fill_stat 时传入目录 st-&gt;size 的值。 如果对 st-&gt;size 不清楚的话，实际上我们执行 ls -lh 看到的文件和目录大小就是 st-&gt;size 的值： 1234567➜ ceph git:(main) ✗ ls -lhtotal 2.8Mdrwxr-xr-x 2 root root 4.0K Mar 1 23:55 admin-rw-r--r-- 1 root root 322 Dec 3 20:34 AUTHORSdrwxr-xr-x 2 root root 4.0K Dec 3 20:34 bindrwxr-xr-x 19 root root 4.0K Feb 12 12:30 build-rw-r--r-- 1 root root 80K Mar 1 23:55 ceph.spec.in 对于文件这里 st-&gt;size 就是 in-&gt;size， client_dirsize_rbytes 这个选项对文件也没有什么影响，对于目录的话 st-&gt;size 在 xfs 等本地文件系统中填充的同样是 inode 的大小，通常就是一个 block 也就是 4k，而在 CephFS 中默认也就是开启 client_dirsize_rbytes 时 st-&gt;size 填充的是 rstat 中递归统计的文件大小总和，如果没有开启的话这里填充的是目录中当前一层的 dentry 个数 12345[root@An1Annaaq4 fuse]# ls -lharttotal 5.0Gdrwxr-xr-x 28 root root 1.5G Jan 10 18:09 annaDirdrwxr-xr-x 2 root root 110G Jan 13 15:06 ivandrwxr-xr-x 102 root root 3.9T Jan 25 10:38 fsstress-test-2 很明显这样对我们平时使用有很大的便利，最直接的就是不用再 dh -su . 了直接就能看总占用，但是关键就在于 rstat 这个东西由 inest 保护，它不是实时更新的，这一点也比较好理解毕竟 rstat 如果稍有改动就要 frozen 目录然后一路更新到根的话那也就没有什么性能可言了 但是这里就有一个问题， tar 这类应用的行为会在压缩开始前拿一把目录的 size 和 mtime，然后在压缩完成后再拿一次并两相比较，如果产生了变化就会像我们遇到的一样提示 File changed as we read it，正常来说在本地文件系统中目录的 size 是不会变的都是 4k，只有 mtime 会因为我们对目录的改动而变化，但是在 CephFS 中反而 mtime 由于受到 filelock 的保护所以导致 getattr 时取到了预期的值，反而 size 也就是 rstat-&gt;rbytes 受到 inest 保护，而 getattr 并不会检查 inest 锁的状态，所以有可能第一次拿回一个滞后的值，而在压缩完成后第二次 getattr 时又拿到了一个更新完成的值，这就导致了问题的产生。 解决办法知道了问题产生的原因的话解决办法也比较简单，将 client_dirsize_rbytes 这项配置设置为 false 即可。不过要注意这样的话我们就没法通过 ls -lh 直接查看目录递归统计值了，只能通过 getfattr 等其他手段来看。 最后小小的吐槽社区 Tracker 上又好几次改 QA 脚本遇到了这个问题，结果最后都没有人看问题的原因，只是回退了脚本。","tags":["Ceph"]},{"title":"Ceph MDS Stuck in Client Replay 问题分析","path":"/posts/2094044811/","content":"最近一直在做 MDS 高可用方面的工作，发现 MDS （带 IO）重启时可能会长时间卡在 Client Replay 状态。这里对问题的原因做了一下分析，并给出了现有的以及未来的解决办法，希望能对大家有所帮助。 问题现象梳理通过 ceph -s 看到 MDS 的状态长时间在 client replay 状态不变化： 1234[root@node2 ~]# ceph -s cluster: ... mds: cephfs: 1/1 &#123;0=node2=up:clientreplay&#125; 查看 MDS 状态会发现 clientreplay_queue 不为空，但是如果打开 MDS 日志会发现 MDS 什么都没做（除了心跳）： 12345678ceph tell mds.ocs-storagecluster-cephfilesystem:0 status&#123; ... &quot;clientreplay_status&quot;: &#123; &quot;clientreplay_queue&quot;: 125048, &quot;active_replay&quot;: 0 &#125;,&#125; 原因分析那这里我们要分析这个问题，就要先知道 MDS 在 Client Replay 阶段做了什么。 首先我们知道 MDS 在启动过程中在 Replay 阶段完成以后（多 MDS 要在 Resolve 阶段以后）会进入 Reconnect 阶段，这个阶段顾名思义会等待客户端进行重连，这也是 MDS 在进入 Active 状态之前唯一能接收客户端请求的阶段，因此客户端会在这个阶段通过 Client::send_reconnect 向 MDS 发送 unsafe_requests, old_requests 以及 client_reconnect 消息, 其中 unsafe_requests 会通过 enqueue_replay 加入 replay_queue 中，old_requests 则会加入 waiting_for_active 等待 MDS 到 active 再处理，client_reconnect 消息则是客户端向 MDS 发送的最后一条消息表示客户端重连完成了（如果 MDS 没有收到这条消息就会把客户端 kill 掉） 12345678void Server::dispatch(const cref_t&lt;Message&gt; &amp;m)&#123; ... if (queue_replay) &#123; req-&gt;mark_queued_for_replay(); mds-&gt;enqueue_replay(new C_MDS_RetryMessage(mds, m)); return; &#125; 接着 MDS 到达 Client Replay 阶段之后就会从 replay_queue 中依次取出刚刚插入的消息并处理，如果一切正常的话每条消息都会在处理完成后 Server::journal_and_reply 或者 Server::reply_client_request 中通过 queue_one_replay 取出下一条消息并处理。 但问题在于并不是每一种情况 MDS 都能 cover 到，首先在任何情况下 Client 都有可能掉线，这导致 MDS 可能在任何时刻 kill_session （一个比较常见的情况是 ganesha 在 client_metadata 里设置了 timeout 所以没有在 Reconnect 阶段 kill_session） 那么如果处理消息时 client session 被 kill 掉又会发生什么呢，正常情况下在 Server::handle_client_request 中如果发现这个 session 被 kill 了那么会 queue_one_replay 处理下一个消息： 1234567void Server::handle_client_request(const cref_t&lt;MClientRequest&gt; &amp;req)&#123; if (!session) &#123; if (req-&gt;is_queued_for_replay()) mds-&gt;queue_one_replay(); return; &#125; 但是如果这个消息此时不是刚开始处理的话就会遇到问题了，假设此前处理请求时候,需要拿锁 Server::acquire_locks： 12022-03-15 12:22:40.185171 7f3e57e90700 10 mds.0.locker wrlock_start (inest sync dirty) on ... 这里拿 wrlock 想要把 inest 锁从 sync 状态转成 lock 状态，但是因为此时 inest 锁状态是 dirty 的，因此需要通过 scatter_writebehind 刷一把 journal，并 WAIT_STABLE: 123456789bool Locker::wrlock_start(const MutationImpl::LockOp &amp;op, MDRequestRef&amp; mut)&#123; ... dout(7) &lt;&lt; &quot;wrlock_start waiting on &quot; &lt;&lt; *lock &lt;&lt; &quot; on &quot; &lt;&lt; *lock-&gt;get_parent() &lt;&lt; dendl; lock-&gt;add_waiter(SimpleLock::WAIT_STABLE, new C_MDS_RetryRequest(mdcache, mut)); nudge_log(lock); return false;&#125; 这里注意 add_waiter 设置的回调是 C_MDS_RetryRequest 和之前加入 replay_queue 时的 C_MDS_RetryMessage 是不一样的，这里就是问题的关键。 当 scatter_writebehind 完成之后由 scatter_writebehind_finish 调到 C_MDS_RetryRequest::finish 12345void C_MDS_RetryRequest::finish(int r)&#123; mdr-&gt;retry++; cache-&gt;dispatch_request(mdr);&#125; 这里直接进入了 MDCache::dispatch_request: 1234void MDCache::dispatch_request(MDRequestRef&amp; mdr)&#123; if (mdr-&gt;client_request) &#123; mds-&gt;server-&gt;dispatch_client_request(mdr); 可以看到这里没有走 Server::handle_client_request 而是直接进入了 Server::dispatch_client_request，在这里对于已经被 kill 掉的 session 的处理就有一个 corner case: 12345678910void Server::dispatch_client_request(MDRequestRef&amp; mdr)&#123; // we shouldn&#x27;t be waiting on anyone. ceph_assert(!mdr-&gt;has_more() || mdr-&gt;more()-&gt;waiting_on_peer.empty()); if (mdr-&gt;killed) &#123; dout(10) &lt;&lt; &quot;request &quot; &lt;&lt; *mdr &lt;&lt; &quot; was killed&quot; &lt;&lt; dendl; return; &#125; ... 这里可以看到直接 return 掉了而没有进行 queue_one_replay，这就使得 MDS 没有办法继续往下进行了 除了这种情况以外在 MDCache::request_start 失败时也会直接返回而不会有机会 queue_one_replay 如何解决目前遇到这种情况没有其他办法，只能通过重启 MDS 来解决（因为没有机会触发 queue_one_replay） 社区的相关进展这个问题实际上社区很早就发现了， queue_one_replay 这个改动就是 YanZheng 在 6352f181 为了 fix ‘stuck in clientreplay’ 的问题提的，但是实际上就像我上面分析的还有一些 corner case 没有覆盖到，这就导致在一些场景下我们仍会遇到这样的问题。 最新的话是 Patrick 在 #47121 中提了一个改动想统一一下 queue_one_replay 的位置，正好我前两天分析了这一块所以给 Patrick 说了现有的这些可能导致 MDS 卡住的情况，然后后面我会再看一下他提的这个 PR 能不能解决问题，如果可以的话后面合到主线应该就不会出现这种问题了。","tags":["Ceph"]},{"title":"如何实现一个最小堆","path":"/posts/2906464195/","content":"本文介绍如何使用 C++ 实现一个通用的、线程安全的最小堆 顺便一提，写此文的时候突然想到了多年以前（16、17年）盛行的 Mooc，当时国内有很多类似的慕课网站，也有很多优秀的课程，我的基础数据结构和算法就是一位 ACM 铜牌的老师教的，至今受用，但是现在好像很少看到此类网站了，还是比较可惜的。 进入正题，最小堆本身是一个完全二叉树，保证每一个节点都不会大于其左右子节点，要实现一个最小堆的两个核心函数是 sift_up 和 sift_down，用于调整最小堆结构以符合定义。 首先给出 MinHeap 的基本定义和成员变量： 123456template &lt;std::three_way_comparable T&gt;class MinHeap&#123; std::shared_mutex mtx; std::vector&lt;T&gt; vec;&#125;; 这里 vec 用于保存最小堆的各个节点（完全二叉树很适合用数组保存），而 std::three_way_comparable 则是对容器元素类型的 concept 约束，要求元素类型必须是可比较的，毕竟不可比较的话也就没有“最小”的概念了。 sift_down 和 sift_up 是相反的一组用于调整堆的操作，sift_down 用于将一个元素向下调整至满足最小堆的条件，sift_up 则相反，由此可以知道对于任何一个完全二叉数我们只需要上到下执行 sift_down 或者从下到上执行 sift_up 就可以使得堆中的每一个节点都满足最小堆的条件 123 5 sift_down(5) 2 / \\ ==============&gt; / \\ 2 6 sift_up(2) 5 6 可以看到不管是对 2 所在的节点执行 sift_up 还是对 5 所在的节点执行 sift_down 都能完成我们想要的效果。 接着在给出二者的定义前先给出对最小堆的一些基本操作： 123456789101112131415161718192021public:void push (T val)&#123; std::unique_lock lock &#123;mtx&#125;; vec.push_back(val); sift_up(vec.size()-1);&#125;void pop()&#123; assert(!vec.empty()); std::unique_lock lock &#123;mtx&#125;; vec[0] = vec.back(); vec.pop_back(); sift_down(0);&#125;const T&amp; top()&#123; assert(!vec.empty()); std::shared_lock lock &#123;mtx&#125;; return vec.front();&#125; push 和 pop 就是对堆调整以符合最小堆定义的过程，因为每次只有一个节点发生变化（新增或者删除），所以只需要对涉及的节点调整即可 下面给出 sift_up 和 sift_down 的定义如下： 12345678910111213141516171819202122232425262728void sift_up(size_t index)&#123; if (index == 0) return; size_t parent = (index - 1) / 2; if (vec[index] &lt; vec[parent]) &#123; std::swap(vec[index], vec[parent]); sift_up(parent); &#125;&#125;void sift_down(size_t index)&#123; if (index &gt;= vec.size()) return; size_t left = index * 2 + 1; size_t right = index * 2 + 2; std::optional&lt;size_t&gt; swap_to; if (left &lt; vec.size() &amp;&amp; vec[left] &lt; vec[index]) &#123; swap_to = left; &#125; if (right &lt; vec.size() &amp;&amp; vec[right] &lt; vec[index]) &#123; if (!swap_to || vec[right] &lt; vec[swap_to.value()]) &#123; swap_to = right; &#125; &#125; if (swap_to) &#123; std::swap(vec[index], vec[swap_to.value()]); sift_down(swap_to.value()); &#125;&#125; 应该还是比较好理解的，因为只有一个节点不（一定）满足最小堆条件，因此对于 sift_up 只需要判断自身和其父节点的大小关系并交换即可，而对于 sift_down 则是需要选出两个子节点中更小的那个并与之交换。 注意这里的 sift_up 和 sift_down 都是一个递归的过程，直到抵达堆的根节点或者叶子节点 以上我们就已经实现了一个通用的、线程安全的最小堆，这里给出堆的完整定义（为了方便使用包含 dump）： 完整的 MinHeap 实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667template &lt;std::three_way_comparable T&gt;class MinHeap&#123; std::shared_mutex mtx; std::vector&lt;T&gt; vec;public: void push (T val) &#123; std::unique_lock lock &#123;mtx&#125;; vec.push_back(val); sift_up(vec.size()-1); &#125; void pop() &#123; assert(!vec.empty()); std::unique_lock lock &#123;mtx&#125;; vec[0] = vec.back(); vec.pop_back(); sift_down(0); &#125; const T&amp; top() &#123; assert(!vec.empty()); std::shared_lock lock &#123;mtx&#125;; return vec.front(); &#125; void sift_up(size_t index) &#123; if (index == 0) return; size_t parent = (index - 1) / 2; if (vec[index] &lt; vec[parent]) &#123; std::swap(vec[index], vec[parent]); sift_up(parent); &#125; &#125; void sift_down(size_t index) &#123; if (index &gt;= vec.size()) return; size_t left = index * 2 + 1; size_t right = index * 2 + 2; std::optional&lt;size_t&gt; swap_to; if (left &lt; vec.size() &amp;&amp; vec[left] &lt; vec[index]) &#123; swap_to = left; &#125; if (right &lt; vec.size() &amp;&amp; vec[right] &lt; vec[index]) &#123; if (!swap_to || vec[right] &lt; vec[swap_to.value()]) &#123; swap_to = right; &#125; &#125; if (swap_to) &#123; std::swap(vec[index], vec[swap_to.value()]); sift_down(swap_to.value()); &#125; &#125; void dump() &#123; std::shared_lock lock &#123;mtx&#125;; for (auto iter = vec.begin(); iter != vec.end(); ++iter) &#123; std::cout &lt;&lt; *iter; if (std::distance(iter, vec.end()) != 1) &#123; std::cout &lt;&lt; &quot;, &quot;; &#125; else &#123; std::cout &lt;&lt; std::endl; &#125; &#125; &#125;&#125;; 用法示例以及运行结果如下： 123456789101112131415161718auto main(void) -&gt; int &#123; auto heap = MinHeap&lt;int&gt; &#123;&#125;; heap.push(177); heap.push(3); heap.push(110); heap.push(19); heap.pop(); heap.push(21); heap.push(112); heap.push(18); std::cout &lt;&lt; &quot;heap&#x27;s top is &quot; &lt;&lt; heap.top() &lt;&lt; std::endl; heap.dump(); heap.pop(); heap.pop(); std::cout &lt;&lt; &quot;heap&#x27;s top is &quot; &lt;&lt; heap.top() &lt;&lt; std::endl; heap.dump(); return 0;&#125; 运行结果： 1234heap&#x27;s top is 1818, 21, 19, 177, 112, 110heap&#x27;s top is 2121, 112, 110, 177 可以看到不论我们以任何顺序插入和删除元素都能够保证 MinHeap 始终是一个最小堆","tags":["C++"]},{"title":"C++ Coroutine: 通用异步任务 Task","path":"/posts/3692590940/","content":"本文使用协程实现了一个通用的异步任务执行类 Task，支持设置回调函数并将在 Task 完成后执行回调。 Task、TaskPromise 和 TaskAwaiter 覆盖到了大部分的协程执行过程，把这几个类的实现理解了那基本上就可以说已经理解了 c++ 协程的工作方式。 阅读下面这段代码的方式建议通过 main 函数开始，对照运行结果一点一点来看。 Source Code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322#include &lt;iostream&gt;#include &lt;coroutine&gt;#include &lt;functional&gt;#include &lt;exception&gt;#include &lt;optional&gt;#include &lt;atomic&gt;#include &lt;utility&gt;#include &lt;chrono&gt;#include &lt;thread&gt;#include &lt;list&gt;#include &lt;condition_variable&gt;#include &lt;algorithm&gt;#include &lt;atomic&gt;#define debug(info) std::cout &lt;&lt; __LINE__ &lt;&lt; &quot; &quot; &lt;&lt; __func__ &lt;&lt; &quot;: &quot; &lt;&lt; info &lt;&lt; std::endl;template&lt;typename Task&gt;struct Result&#123; // 初始化为默认值 explicit Result() = default; // 当 Task 正常返回时用结果初始化 Result explicit Result(Task &amp;&amp;value) : _value(value) &#123; debug(&quot;Construct from value&quot;); &#125; // 当 Task 抛异常时用异常初始化 Result explicit Result(std::exception_ptr &amp;&amp;exception_ptr) : _exception_ptr(exception_ptr) &#123; debug(&quot;Construct from exception_ptr&quot;); &#125; // 读取结果，有异常则抛出异常 Task get_or_throw() &#123; if (_exception_ptr) &#123; std::rethrow_exception(_exception_ptr); &#125; return _value; &#125; private: Task _value&#123;&#125;; std::exception_ptr _exception_ptr;&#125;;template&lt;template&lt;typename&gt; class Task, typename R&gt;struct TaskAwaiter &#123; explicit TaskAwaiter(Task&lt;R&gt; &amp;&amp;task) noexcept : task(std::move(task)) &#123; debug(&quot;Construct from task&quot;); &#125; TaskAwaiter(TaskAwaiter &amp;&amp;completion) noexcept : task(std::exchange(completion.task, &#123;&#125;)) &#123; debug(&quot;Construct from completion&quot;); &#125; TaskAwaiter(TaskAwaiter &amp;) = delete; TaskAwaiter &amp;operator=(TaskAwaiter &amp;) = delete; constexpr bool await_ready() const noexcept &#123; debug(&quot;&quot;); return false; &#125; void await_suspend(std::coroutine_handle&lt;&gt; handle) noexcept &#123; debug(&quot;&quot;); std::cout &lt;&lt; handle.address() &lt;&lt; std::endl; // 当 task 执行完之后调用 resume task.finally([handle]() &#123; handle.resume(); &#125;); &#125; // 协程恢复执行时，被等待的 Task 已经执行完，调用 get_result 来获取结果 R await_resume() noexcept &#123; debug(&quot;&quot;); return task.get_result(); &#125; private: Task&lt;R&gt; task;&#125;;template&lt;template&lt;typename&gt; class Task, typename ResultType&gt;struct TaskPromise&#123; auto initial_suspend() noexcept &#123; debug(&quot;&quot;); return std::suspend_never&#123;&#125;; &#125; auto final_suspend() noexcept &#123; debug(&quot;&quot;); return std::suspend_always&#123;&#125;; &#125; Task&lt;ResultType&gt; get_return_object() &#123; return Task&#123;std::coroutine_handle&lt;TaskPromise&gt;::from_promise(*this)&#125;; &#125; void unhandled_exception() &#123; debug(&quot;&quot;); std::lock_guard lock(completion_lock); result = Result&lt;ResultType&gt;(std::current_exception()); completion.notify_all(); // 调用回调 notify_callbacks(); &#125; void return_value(ResultType value) &#123; debug(&quot;&quot;); std::lock_guard lock(completion_lock); result = Result&lt;ResultType&gt;(std::move(value)); completion.notify_all(); // 调用回调 notify_callbacks(); &#125; ResultType get_result() &#123; debug(&quot;from TaskPromise&quot;); // 如果 result 没有值，说明协程还没有运行完，等待值被写入再返回 std::unique_lock lock(completion_lock); if (!result.has_value()) &#123; debug(&quot;hasn&#x27;t value now&quot;); // 等待写入值之后调用 notify_all completion.wait(lock); &#125; else &#123; debug(&quot;already has value now&quot;); &#125; // 如果有值，则直接返回（或者抛出异常） return result-&gt;get_or_throw(); &#125; void on_completed(std::function&lt;void(Result&lt;ResultType&gt;)&gt; &amp;&amp;func) &#123; debug(&quot;&quot;); std::unique_lock lock(completion_lock); // 加锁判断 result if (result.has_value()) &#123; debug(&quot;already has value&quot;); // result 已经有值 auto value = result.value(); // 解锁之后再调用 func lock.unlock(); func(value); &#125; else &#123; debug(&quot;waiting for execution&quot;); // 否则添加回调函数，等待调用 completion_callbacks.push_back(func); &#125; &#125; // 注意这里的模板参数 template&lt;typename _ResultType&gt; auto await_transform(Task&lt;_ResultType&gt; &amp;&amp;task) &#123; debug(&quot;&quot;); return TaskAwaiter&lt;Task, _ResultType&gt;&#123;std::move(task)&#125;; &#125; private: // 回调列表，我们允许对同一个 Task 添加多个回调 std::list&lt;std::function&lt;void(Result&lt;ResultType&gt;)&gt;&gt; completion_callbacks; void notify_callbacks() &#123; debug(&quot;&quot;); auto value = result.value(); for (auto &amp;callback : completion_callbacks) &#123; debug(&quot;call callback function from completion_callbacks&quot;); callback(value); &#125; // 调用完成，清空回调 completion_callbacks.clear(); &#125; // 使用 std::optional 可以区分协程是否执行完成 std::optional&lt;Result&lt;ResultType&gt;&gt; result; std::mutex completion_lock; std::condition_variable completion;&#125;;template&lt;typename ResultType&gt;struct Task&#123; // 声明 promise_type 为 TaskPromise 类型 using promise_type = TaskPromise&lt;Task, ResultType&gt;; ResultType get_result() &#123; debug(&quot;from Task&quot;); return handle.promise().get_result(); &#125; Task &amp;then(std::function&lt;void(ResultType)&gt; &amp;&amp;func) &#123; debug(&quot;task id = &quot; + std::to_string(task_id)); std::cout &lt;&lt; handle.address() &lt;&lt; std::endl; handle.promise().on_completed([func](auto result) &#123; try &#123; func(result.get_or_throw()); &#125; catch (std::exception &amp;e) &#123; // 忽略异常 &#125; &#125;); return *this; &#125; Task &amp;catching(std::function&lt;void(std::exception &amp;)&gt; &amp;&amp;func) &#123; debug(&quot;task id = &quot; + std::to_string(task_id)); handle.promise().on_completed([func](auto result) &#123; try &#123; // 忽略返回值 result.get_or_throw(); &#125; catch (std::exception &amp;e) &#123; func(e); &#125; &#125;); return *this; &#125; Task &amp;finally(std::function&lt;void()&gt; &amp;&amp;func) &#123; debug(&quot;task id = &quot; + std::to_string(task_id)); std::cout &lt;&lt; handle.address() &lt;&lt; std::endl; handle.promise().on_completed([func](auto result) &#123; func(); &#125;); return *this; &#125; explicit Task(std::coroutine_handle&lt;promise_type&gt; handle) noexcept: handle(handle), task_id(cnt) &#123; ++cnt; debug(&quot;Construct from handle, task id = &quot; + std::to_string(task_id)); std::cout &lt;&lt; handle.address() &lt;&lt; std::endl; &#125; Task(Task &amp;&amp;task) noexcept: handle(std::exchange(task.handle, &#123;&#125;)), task_id(cnt) &#123; ++cnt; debug(&quot;Move Construct from task, task id = &quot; + std::to_string(task.task_id) &lt;&lt; &quot;-&gt;&quot; &lt;&lt; std::to_string(task_id)); std::cout &lt;&lt; handle.address() &lt;&lt; std::endl; &#125; Task(Task &amp;) = delete; Task &amp;operator=(Task &amp;) = delete; ~Task() &#123; if (handle) handle.destroy(); &#125; private: std::coroutine_handle&lt;promise_type&gt; handle; static std::atomic&lt;int&gt; cnt; const int task_id;&#125;;template &lt;typename T&gt;std::atomic&lt;int&gt; Task&lt;T&gt;::cnt = 0;Task&lt;int&gt; simple_task2() &#123; debug(&quot;task 2 start ...&quot;); using namespace std::chrono_literals; std::this_thread::sleep_for(1s); debug(&quot;task 2 returns after 1s.&quot;); co_return 2;&#125;Task&lt;int&gt; simple_task3() &#123; debug(&quot;in task 3 start ...&quot;); using namespace std::chrono_literals; std::this_thread::sleep_for(2s); debug(&quot;task 3 returns after 2s.&quot;); co_return 3;&#125;Task&lt;int&gt; simple_task() &#123; debug(&quot;task start ...&quot;); auto result2 = co_await simple_task2(); debug(&quot;returns from task2: &quot; + std::to_string(result2)); auto result3 = co_await simple_task3(); debug(&quot;returns from task3: &quot; + std::to_string(result3)); co_return 1 + result2 + result3;&#125;int main() &#123; auto simpleTask = simple_task(); std::cout &lt;&lt; &quot;======================&quot; &lt;&lt; std::endl; simpleTask.then([](int i) &#123; debug(&quot;simple task end: &quot; + std::to_string(i)); &#125;).catching([](std::exception &amp;e) &#123; debug(&quot;error occurred&quot; + std::string&#123;e.what()&#125;); &#125;); std::cout &lt;&lt; &quot;======================&quot; &lt;&lt; std::endl; try &#123; auto i = simpleTask.get_result(); debug(&quot;simple task end from get: &quot; + std::to_string(i)); &#125; catch (std::exception &amp;e) &#123; debug(&quot;error: &quot; + std::string&#123;e.what()&#125;); &#125; return 0;&#125; 运行结果分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110ASM generation compiler returned: 0Execution build compiler returned: 0Program returned: 0// 这里通过 auto simpleTask = simple_task(); 构建第一个 Task1 0x5562ea17feb0250 Task: Construct from handle, task id = 00x5562ea17feb0// 构建协程类第一步 initial_suspend100 initial_suspend: // 这里因为 suspend_never 所以继往下走295 simple_task: task start ...// 通过 auto result2 = co_await simple_task2(); 构建第二个 Task2 0x5562ea180fe0250 Task: Construct from handle, task id = 10x5562ea180fe0100 initial_suspend: 279 simple_task2: task 2 start ...282 simple_task2: task 2 returns after 1s.// 一直走到 co_return 2; 进入 TaskPromise::return_value126 return_value: // 通过 result = Result&lt;ResultType&gt;(std::move(value)); 用 value(2) 构建了一个 Result26 Result: Construct from value// 中间调了 completion.notify_all(); 但是这里没有 wait// 继续在 TaskPromise::return_value 中 notify_callbacks();181 notify_callbacks: // co_return 之后这个协程 0x5562ea180fe0 就结束了 (还没释放，因为 finial 是 suspend_always)105 final_suspend: // 这里把 co_return 的结果给到 Task1 的 TaskPromise170 await_transform: // await_transform 返回一个 TaskAwaiter, 把刚刚的 Task2 存起来了（移动拷贝）257 Task: Move Construct from task, task id = 1-&gt;20x5562ea180fe055 TaskAwaiter: Construct from task// 这里先判断 ready, 直接返回 false70 await_ready: // false 的情况下需要 suspend (注意这里传入的参数 handle 是 Task1 的 handle)76 await_suspend: 0x5562ea17feb0// 这里 task.finally([handle]() &#123; 就是刚刚存起来的 Task2// 传了一个回调去 handle.resume();241 finally: task id = 20x5562ea180fe0// handle.promise().on_completed([func](auto result) &#123; func(); &#125;);149 on_completed: // 这里判断了一下 result 有没有值, 那么这里因为是 Task2 的 Promise // 所以 result 在刚刚 return_value 填充上了153 on_completed: already has value// 这里是通过刚刚的回调调到的 Task1 Promise 的 handle.resume()87 await_resume: // 这里 return task.get_result(); 这里的 task 是刚刚传到 TaskAwaiter 的 Task2207 get_result: from Task136 get_result: from TaskPromise// 这里还是一样的 result 是 value(2)144 get_result: already has value now// 这里因为 TaskAwaiter 在 finally 中 resume 了 Task1 的 handle 所以继续进行297 simple_task: returns from task2: 2// 通过 auto result3 = co_await simple_task3(); 构建 Task3 0x5562ea180fe0250 Task: Construct from handle, task id = 30x5562ea180fe0// 后面 Task3 执行和返回的过程大部分都一样的100 initial_suspend: 287 simple_task3: in task 3 start ...290 simple_task3: task 3 returns after 2s.126 return_value: 26 Result: Construct from value181 notify_callbacks: 105 final_suspend: 170 await_transform: 257 Task: Move Construct from task, task id = 3-&gt;40x5562ea180fe055 TaskAwaiter: Construct from task70 await_ready: 76 await_suspend: 0x5562ea17feb0241 finally: task id = 40x5562ea180fe0149 on_completed: 153 on_completed: already has value87 await_resume: 207 get_result: from Task136 get_result: from TaskPromise144 get_result: already has value now// 一直到这里同样 Task1 通过 Task3 的回调被 resume 继续执行299 simple_task: returns from task3: 3// 这里 co_return 1 + result2 + result3; 调到 return_value126 return_value: // 这里和前面一样同样构建了 Result26 Result: Construct from value181 notify_callbacks: // 这里 Task 也走完了, suspend105 final_suspend: ======================// 刚刚所有的过程只在声明了 simpleTask 之后就执行完了// 接着调用 simpleTask.then()213 then: task id = 00x5562ea17feb0// 那这里同样之前已经有值了149 on_completed: 153 on_completed: already has value// 这里通过 on_completed 调回到 lambda 回调 debug(&quot;simple task end: &quot; + std::to_string(i));306 operator(): simple task end: 6// 这里 catch 了一下异常，但是因为没有异常所以没有触发 debug(&quot;error: &quot; + std::string&#123;e.what()&#125;);227 catching: task id = 0149 on_completed: 153 on_completed: already has value======================// try-catch 里的逻辑也类似207 get_result: from Task136 get_result: from TaskPromise144 get_result: already has value now313 main: simple task end from get: 6 References 渡劫 C++ 协程（4）：通用异步任务 Task","tags":["C++","Coroutine"]},{"title":"Ceph Client 中的 LRU 设计","path":"/posts/3763995770/","content":"在 Ceph 中 Client 和 MDS 部分的 lru 实现和淘汰策略稍有不同，但作用都是用来管理 dentry 等结构，方便及时将不使用的文件和目录清出内存，这里主要关注 lru 的实现而不关注 dentry 的淘汰时机。 首先看下 LRU 的结构： 1234567// lru.hclass LRU &#123;// ...private: using LRUList = xlist&lt;LRUObject*&gt;; LRUList top, bottom, pintail;&#125; 这里和大部分 LRU 的实现类似， ceph 也使用一个双向链表作为 lru 的基础结构（也就是这里的 xlist，基本可以当作一个双向链表看待）。只不过 ceph 同时使用了三个链表 top、bottom 和 pintail 来细化 lru 的不同位置。 其中 top 和 bottom 比较好理解就是一般 lru 实现中的首尾两端，越靠近 top.front() 则说明最近越多使用，越靠近 bottom.back() 则说明最近越少使用，可以被淘汰。 pintail 稍微特殊一点，在此链中的 dentry 表示正在有人使用（可能是外部应用持有文件的 filehandle，因此随时有可能访问），因此不会被 从 lru 链中淘汰，即使最近没有人使用。 123void lru_insert_top(LRUObject *o);void lru_insert_mid(LRUObject *o);void lru_insert_bot(LRUObject *o); 这里就对应将一个 lru object 插入 lru 链的三个位置（从外部看 lru 是一个链，内部才分为 top 或者 bottom），实际对应就是 123top.push_front(&amp;o-&gt;lru_link); // 将 object 插入 top 头部bottom.push_front(&amp;o-&gt;lru_link); // 将 object 插入 bottom 头部bottom.push_back(&amp;o-&gt;lru_link); // 将 object 插入 bottom 尾部，也是最接近淘汰的位置 另外之前已经说了 xlist 的实现属于是一个双向链表的变种，这里也简单看一下，以 top.push_front 为例： 1234void push_front(item *i) &#123; if (i-&gt;_list) i-&gt;_list-&gt;remove(i);// ... 首先就是如果这个 item（也就是一个 lru object） 已经在某一个链上了，那么先通过 remove 把这个 item 从先前的链上摘下来，这个实现可以避免一个 item 挂在多个链上 123// ... i-&gt;_list = this;// ... 接着将自己的 _list 指向 this，表示 item 的所属链表（也就是 top） 1234// ... i-&gt;_next = _front; i-&gt;_prev = 0;// ... 这里就是首先把 item 挂到链表上，因为是 push_front 所以 _prev 为空， _front 是整个链表的头部。 12345678// ... if (_front) _front-&gt;_prev = i; else _back = i; _front = i; _size++;\t&#125; 同时要更新一下链表的头尾指针，以及整个链表的长度。 接着，必要时 client 通过 lru_get_next_expire 获取最近最少使用的 object 进行 trim，这个过程实际上就是从 lru 中拿出 bottom.back() 12345678910LRUObject *lru_get_next_expire() &#123;\tadjust();\t// look through tail of bot\twhile (bottom.size()) &#123; LRUObject *p = bottom.back();\tif (!p-&gt;lru_pinned) return p;\t// move to pintail\tpintail.push_front(&amp;p-&gt;lru_link);&#125; 这里 adjust 就是把整个 lru 链中 object 数量的 60%（实际要减去 pinned object）放入 top，剩余部分放入 bottom 接着直接从 bottom 拿出末端 object，如果这个 object 没有 pinned 则直接将其返回，表示此 object 可以被 trim pin 和 unpin 则是一个特殊的过程， pin 一个指定的 dentry 时我们只需要将对应的 object 从 top 或者 bottom 上移出到 pintail 链上（无关乎首尾，因为 pintail 上的 dentry 都不能 trim），而 unpin 则只需要将其 pintail 中移出到 bottom 的尾部即可 整个 lru 设计相对来说比较容易理解，也非常易于使用。 对于一个特定的 dentry，我们会在初次获取到其 object 时可以将其放至 lru mid，初次使用时将其放至 lru top，接着如果没有使用会随着 lru 的 adjust 逐渐移动到 bottom 然后被 client 取出作为 expired object 淘汰掉，而如果这时通过 pin 加入 pintail 的话就会一直等到 unpin 之后再放到 bottom.back 等待 trim","tags":["Ceph"]},{"title":"C++20 Asio With Boost 获取 B 站徽章（直播间牌子）","path":"/posts/173569173/","content":"本文介绍了如何使用 C++20 with Asio (Boost 版本) 完成一个简单的客户端用于获取 B 站徽章。 强烈推荐昨天发现的一个视频 《Why C++20 is the Awesomest Language for Network Programming》，可以去油管上搜一下，总时长一个小时，比较长但是讲的很好，听的巨舒服，上次有这种感觉还是听那个 c10k 问题的视频。 总之先看一下程序执行的效果： 123➜ bili-medal git:(master) ./build/main &#123;my_cookie&#125;当前登录账号的 mid 为 29248492当前登陆账号所佩戴的直播间徽章为 &quot;ASAKI&quot;, 等级为 15 级 贴一下主要逻辑： 123456789101112131415161718192021222324252627282930313233343536373839404142awaitable&lt;void&gt; start(asio::io_context &amp;ctx, const std::string &amp;cookie)&#123; const std::string host = &quot;api.bilibili.com&quot;; // 解析域名 auto [e1, endpoint] = co_await tcp::resolver(ctx).async_resolve(host, &quot;https&quot;, use_nothrow_awaitable); // ssl_ctx 用于建立 https 连接 ssl::context ssl_ctx &#123;ssl::context::sslv23&#125;; ssl_socket socket &#123;ctx, ssl_ctx&#125;; static_assert(std::is_same_v&lt;std::decay_t&lt;decltype(socket.next_layer())&gt;, tcp::socket&gt;); // 建立 tcp 连接 ssl_ctx.set_default_verify_paths(); auto [e2, _] = co_await asio::async_connect(socket.lowest_layer(), endpoint, use_nothrow_awaitable); // https 握手，建立 https 连接 auto [e3] = co_await socket.async_handshake(ssl::stream_base::client, use_nothrow_awaitable); std::string mid; &#123; // 创建并发送一个 api 请求到 B 站服务器，获得返回结果并解析 mid 字段 auto request = make_request(http::verb::get, host, &quot;/x/web-interface/nav&quot;, cookie); auto response = co_await async_send_request(socket, request); auto data = json::parse(beast::buffers_to_string(response.body().data())); mid = data[&quot;data&quot;][&quot;mid&quot;].dump(); &#125; std::cout &lt;&lt; &quot;当前登录账号的 mid 为 &quot; &lt;&lt; mid &lt;&lt; std::endl; std::string medal_id, medal_name, medal_level; &#123; // 创建并发送一个 api 请求到 B 站服务器，获得返回结果并解析直播间徽章 auto request = make_request(http::verb::get, host, &quot;/x/space/acc/info?mid=&quot;+mid, cookie); auto response = co_await async_send_request(socket, request); auto data = json::parse(beast::buffers_to_string(response.body().data())); medal_id = data[&quot;data&quot;][&quot;fans_medal&quot;][&quot;medal&quot;][&quot;medal_id&quot;].dump(); medal_level = data[&quot;data&quot;][&quot;fans_medal&quot;][&quot;medal&quot;][&quot;level&quot;].dump(); medal_name = data[&quot;data&quot;][&quot;fans_medal&quot;][&quot;medal&quot;][&quot;medal_name&quot;].dump(); &#125; std::cout &lt;&lt; &quot;当前登陆账号所佩戴的直播间徽章为 &quot; &lt;&lt; medal_name &lt;&lt; &quot;, 等级为 &quot; &lt;&lt; medal_level &lt;&lt; &quot; 级&quot; &lt;&lt; std::endl;&#125; 整体流程我觉得是比较清晰的，基本上对 https 请求过程有所了解的话应该能容易能理解这里做了什么。 其中 async_resolve async_connect async_handshake 都是 Asio 提供的协程版本的异步函数，通过使用这些函数我们可以不必设置回调函数而是可以以同步方式书写代码，非常好用。 make_request 和 async_send_request 则是我们自己定义的两个函数，实现如下： 1234567891011121314151617181920212223http::request&lt;http::empty_body&gt; make_request(http::verb method, std::string host, std::string target, std::string cookie)&#123; http::request&lt;http::empty_body&gt; request; request.method(method); request.set(http::field::host, host); request.target(target); request.set(http::field::user_agent, BOOST_BEAST_VERSION_STRING); request.set(http::field::cookie, &quot;SESSDATA=&quot; + cookie); return request;&#125;awaitable&lt;http::response&lt;http::dynamic_body&gt;&gt; async_send_request(ssl_socket &amp;socket, http::request&lt;http::empty_body&gt; request)&#123; auto [e1, wbytes] = co_await http::async_write(socket, request, use_nothrow_awaitable); http::response&lt;http::dynamic_body&gt; response; beast::flat_buffer buffer; auto [e2, rbytes] = co_await http::async_read(socket, buffer, response, use_nothrow_awaitable); co_return response;&#125; 逻辑也比较简单，因为是 api 调用所以我们通过 async_write 发送请求之后直接通过 async_read 读取返回结果，注意这里的返回值 awaitable&lt;&gt; 是一个协程包装器类型，这使得我们的 async_send_request 函数实际成为了一个协程工厂，这样我们就可以通过 co_await 来调用了 另外对于返回的数据进行处理的部分则是使用了 nlohmann_json 完成 综上我们就实现了一个简单的客户端用于通过 cookie 获取用户所佩戴的直播间牌子的功能，这里可以看到我们实际上使用协程并不能够对程序的效率有多大的提高（单线程客户端，还是顺序执行的过程），但是更大的意义在于我们用同步方式写异步代码使得代码的可读性非常高，这是非常酷的一件事，在大型项目（尤其是服务端开发）中十分重要。","tags":["C++","Asio"]},{"title":"ChatGPT 注册教程","path":"/posts/3773623110/","content":"ChatGPT 推出已经有一段时间了，但是由于地区政策国内很多小伙伴都还没有能有机会区体验 AI 的魅力，这里就给大家介绍一下如何注册和使用 ChatGPT 前期准备要使用 ChatGPT, 你需要如下几样东西： 123- 一个科学且靠谱的上网办法- 一个邮箱用于接收验证码- 一个 Google 账号用于快速注册登录 (可选) 接着切换你的代理节点，尝试打开 ChatGPT 的登陆页面 如果你看到如下画面： 恭喜你此节点可以用于访问 ChatGPT，但如果 ChatGPT 提示 Access Deined,那就说明此节点被列入黑名单，需要换一个节点再试。 另外需要注意的一点是，尽量优先尝试使用非香港节点登录 ChatGPT 确认节点是否可用的原因是等下会用到一个接码平台用于注册 ChatGPT，要花几块钱，这里如果没有可用节点的话，等下充值甚至注册了没法用就很难受。 开始注册前期准备完成以后点击 Sign up 进入 ChatGPT 的注册页面，显示如下： 如果你恰好有一个 Google 账号，那么恭喜你，可以免去邮箱验证直接进入手机号验证环节，如果没有的话那就需要先验证邮箱 邮箱验证在 Email address 一栏中输入你的邮箱地址，点击 Continue，输入密码： 输入密码后继续点击 Continue ChatGPT 会提示你 Verify your email，这时你就可以打开你的邮箱点击 Verify email address 来验证了。 验证完成之后会提示你输入姓名，这里随便输一个就好： 接着 ChatGPT 会提示你需要绑定一个手机号 验证手机号邮箱验证完成或直接点击 Continue With Google 之后进入验证手机号的界面如下： 这里我们注意国内的手机号是不可用的，所以我们需要一个接码平台来接收验证码。 点击打开接码平台 sms-activate 的 &gt;&gt;&gt;主页面&lt;&lt;&lt; 并点击注册： 注意这里邮箱最好填 Gmail, 不行的话再填 QQ 邮箱，我自己第一次注册用的是 icloud 的一次性邮箱结果就没收到确认邮件。 查收邮件后点击确认即可，回到 sms-activate, 在右上角位置点击充值: 这一步之前是没有最小额度限制的，但是现在最少充 1 美元 (大概7块钱)，而我们注册 ChatGPT 只需要 3 块，剩下的钱你可以注册个什么其他账号玩一玩。 支付方式是支持支付宝的，直接扫码支付就可以。 接着到左边的搜索框中搜索 openai 并点击: 这里找一个最便宜的印尼就好了，点击右边的那个购物车（之前阿三貌似只要 15 卢布，用户多了以后涨价了） 接着 sms-activate 就会给你显示一个电话号码，有效期是 30 分钟，你就可以用这个电话返回到 ChatGPT 填入并点击 Send Code (注意点击国旗选择正确的国家和地区，印尼是 I 开头的) 如果这个时候 ChatGPT 提示你账号可疑不能发送验证码，那么没有关系，回到 sms-activate 点击手机号右边的叉取消订单（扣除的卢布会立即返还），然后再点击购物车图标重新换一个号码就好。 开始使用注册完成后在下方输入框输入你要说的话就可以和 ChatGPT 开始愉快的聊天了 &#x3D;v&#x3D; 你可以用它来辅助学习： 不过以我这段时间的经验来看， ChatGPT 只能是做到一个辅助的作用，由于数据集过于庞大和复杂，在涉及到一些比较复杂的问题时 ChatGPT 会混淆不同版本实现的差异甚至自己创造一些源码里不存在的函数和变量（说人话就是一本正经的跟你胡说八道），比如下面这样： 我猜 ChatGPT 可能是日裔（ 祝大家玩的开心！","tags":["Other"]},{"title":"如何向 STL 算法中传入重载函数","path":"/posts/781272769/","content":"本文分享如何向 STL 算法中传入重载函数，来自 Jonathan 2017 年在 Fluent C++ 上发起的一个挑战 123The STL is a fantastic tool to make your code more expressive and more robust. If you’re a C++ developer and want to become proficient, it is essential that you learn the STL.But there is one case where we can’t apply STL algorithms right out of the box: when the function passed has overloads. 大概意思是说 STL 本身非常好用，但是如果传递一个具有重载的函数就会使得 STL 失效。 举个例子，如果我们定义了一个函数 func 以及一个 vector 数组如下： 12345void func (int &amp;i) &#123; ++i;&#125;std::vector&lt;int&gt; numbers = &#123;1, 2, 3, 4, 5&#125;; 那么我们可以通过 for_each 使 numbers 中的每个数字加一： 1std::for_each(begin(numbers), end(numbers), func); 但是这时如果我们还有一个同名的重载函数： 1void func (std::string &amp;s); 就会导致刚刚的代码编译失败, 因为编译器无法推断我们要使用哪一个版本： 1234567main.cpp: In function &#x27;int main()&#x27;:main.cpp:20:50: error: no matching function for call to &#x27;for_each(std::vector&lt;int&gt;::iterator, std::vector&lt;int&gt;::iterator, &lt;unresolved overloaded function type&gt;)&#x27; std::for_each(begin(numbers), end(numbers), func); ^/usr/local/include/c++/7.1.0/bits/stl_algo.h:3878:5: note: template argument deduction/substitution failed:main.cpp:20:50: note: couldn&#x27;t deduce template parameter &#x27;_Funct&#x27; std::for_each(begin(numbers), end(numbers), func); 这时我们只能通过指定 func 的类型来避免这个错误： 1std::for_each(begin(numbers), end(numbers), static_cast&lt;void(*)(int&amp;)&gt;(func)); 这显然并不美观，也会使得代码的可读性下降，于是 Jonathan 希望大家能够用更好的方式来解决这个问题不知道大家现在是否有自己的想法呢,如果你也想挑战一下这个问题，不妨暂停一下，来一个简单的头脑风暴，说不定你的方案更加优秀。 倒计时 3 2 1 答案揭晓！ 首先说一下我个人的想法，我的想法非常简单，首先这里编译报错是因为 for_each 函数无法判断这里所用到的 func 应该使用哪一个版本，即使这对我们来说非常直观 (容器内是 int 那么就应该调用 int 版本的重载，string 亦然)。 那么为了帮助确定重载版本，我们可以使用一个 lambda 函数代替 func 作为 for_each 的处理函数，而 lambda 的实现方式允许我们拿到 处理的参数类型并进一步确定所调用的 func 版本 1std::for_each(begin(numbers), end(numbers), [](auto i)&#123; return func(i); &#125;); 如果你的想法也和我一样，恭喜你也同时了 Jonathan 的称赞，他评价这种解决方案 It’s not as generic as the the previous solution, but it does all that is necessary for simple cases这并非通用的解决方案，但它能很好的胜任大多数不复杂的场景 接下来让我们看一下本次挑战的最终获胜答案，来自 Vittorio Romeo: 1234567// C++ requires you to type out the same function body three times to obtain SFINAE-friendliness and// noexcept-correctness. That&#x27;s unacceptable.#define RETURNS(...) noexcept(noexcept(__VA_ARGS__)) -&gt; decltype(__VA_ARGS__)&#123; return __VA_ARGS__; &#125;// The name of overload sets can be legally used as part of a function call - we can use a macro to// create a lambda for us that &quot;lifts&quot; the overload set into a function object.#define LIFT(f) [](auto&amp;&amp;... xs) RETURNS(f(::std::forward&lt;decltype(xs)&gt;(xs)...)) 怎么样，你的脑袋有没有被‘轻轻敲醒’的感觉，反正与我而言乍一看这段代码直接就是一个当头棒喝。不过当你沉下心一点点分析这段代码，你就会越来越被这代码里精妙的设计吸引。 为了理解这部分代码，我们可以先尝试对 LIFT(func) 进行展开，结果如下: 1234[](auto&amp;&amp;... xs) noexcept(noexcept(func(::std::forward&lt;decltype(xs)&gt;(xs)...))) -&gt; decltype(func(::std::forward&lt;decltype(xs)&gt;(xs)...))&#123; return func(::std::forward&lt;decltype(xs)&gt;(xs)...);&#125; 怎么样,是不是突然就有了一种熟悉的感觉。没错，其实这也是一个 lambda 函数，只是相比于我们的简略版本考虑的更多的情况，接下来让我们一点一点分析。 1234[] (auto&amp;&amp;... xs) /*省略*/&#123; return func(::std::forward&lt;decltype(xs)&gt;(xs)...);&#125; 除开省略部分我们可以看到就是一个完美转发并支持变长参数，这使得 LIFT 可以适用于多个参数的函数并且不改变其传入参数的类型。 接着看省略部分，也就是 RETURNS 部分的内容: 12/*省略*/ noexcept(noexcept(func(::std::forward&lt;decltype(xs)&gt;(xs)...))) -&gt; decltype(func(::std::forward&lt;decltype(xs)&gt;(xs)...))/*省略*/ 包含两部分内容 noexcept(...) 和 -&gt; decltype(...): 后者指定了 LIFT 的返回类型，在我们的简单方法中没有指定，因此默认会使用 -&gt; auto，这可能会使得由 func 返回的值类型被改变，考虑如下示例： 123456789101112131415161718192021222324252627282930int&amp; func_return_value (int&amp; i) &#123; return i;&#125;void use_value(int&amp; i) &#123; std::cout &lt;&lt; &quot;I got a l value i:&quot; &lt;&lt; i &lt;&lt; &amp;i &lt;&lt; std::endl;&#125;void use_value(int&amp;&amp; i) &#123; std::cout &lt;&lt; &quot;I got a r value i:&quot; &lt;&lt; i &lt;&lt; &amp;i &lt;&lt; std::endl;&#125;auto x = [](auto&amp;&amp;... xs) &#123; return func_return_value(::std::forward&lt;decltype(xs)&gt;(xs)...);&#125;;auto x1 = [](auto&amp;&amp;... xs) -&gt; decltype(func_return_value(::std::forward&lt;decltype(xs)&gt;(xs)...))&#123; return func_return_value(::std::forward&lt;decltype(xs)&gt;(xs)...);&#125;;auto main(void) -&gt; int &#123; int a = 3; // I have a value a :30x7ffc9ecd766c std::cout &lt;&lt; &quot;I have a value a :&quot; &lt;&lt; a &lt;&lt; &amp;a &lt;&lt; std::endl; // I got a r value i:30x7ffc9ecd76ac use_value(x(a)); // I got a l value i:30x7ffc9ecd766c use_value(x1(a));&#125; 对示例来说我们希望拿到并传入由 func_return_value 返回的 int&amp; 类型变量，但是由于 x 中的默认返回类型为 auto，因此导致其示例化了一个新的临时变量并作为右值返回。 接着思考另外一个问题，如果我们使用 -&gt;decltype(auto) 代替 -&gt;auto 呢，是否可以达到我们想要的目标, 又会有什么问题呢。 Jonathan 也帮助我们分析了这一问题，并指出 decltype(expr) 的版本具有更好的 SFINAE 亲和性, Guillaume Racicot 在 stackoverflow 上也讨论了这个问题。 实际上结合 Vittorio 在 C++Now 2017 的演讲，我更偏向于这里使用 -&gt;decltype(auto) 也能同样出色的完成任务，但是 RETURNS 中的用法可以带给我们再其他更多场景下使用的启发。 接着 LIFT 的最后一部分 noexcept 则是根据传入的 func 设置是否抛出异常 综上，希望大家能够收获一点有用的东西！ References: Can You Wield C++ Function Overloading Like Jon Snow Wields Longclaw?","tags":["C++"]},{"title":"RAC 集群访问过程分析","path":"/posts/4037772901/","content":"本文介绍了通过 WireShark 分析的在 RAC11gR2 集群访问中连接建立的过程 环境 环境： 主机：192.168.40.37 SCAN节点：192.168.40.234 节点1：192.168.40.230 vip：192.168.40.231 节点2：192.168.40.232 vip：192.168.40.233 连接过程 主机与scan节点进行三次握手，建立tcp连接（目的端口1521） 主机向scan节点发送Oracle-TNS（Transparent Network Substrate Protocol）Request，主要携带信息如下： 1234567891011121314Packet Type: Connect (1)Connect Version: 314 Service Options: 0x0041 Session Data Unit Size: 8192 Maximum Transmission Data Unit Size: 32767 NT Protocol Characteristics:0xc60e, Hangon to listener connect, Confirmed release, Callback IO supported, ASync IO Supported, Generate SIGURG signal, Urgent IO supported, Full duplex IO supported Value of 1 in Hardware: 0100 Connect Flags 0:0x41, NA services wanted Connect Flags 1:0x41, NA services wanted Trace Cross Facility Item 1: 0x00000000 Trace Cross Facility Item 2: 0x00000000 Trace Unique Connection ID: 0x0000000000000000 Connect Data: (DESCRIPTION=(CONNECT_DATA=(SERVICE_NAME=MER)(CID=(PROGRAM=E:\\Navicat?for?Oracle avicat.exe)(HOST=DESKTOP-PSOJPQF)(USER=MER)))(ADDRESS=(PROTOCOL=tcp)(HOST=192.168.40.234)(PORT=1521))) scan返回TNS Response, Redirect消息 scan返回TNS Response, Data，包含指示主机需要请求的host信息，同时填充了要访问的实例名： 1234567Data Data Flag:0x0040, End of File Data Data: (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=192.168.40.231)(PORT=1521))) (DESCRIPTION=(CONNECT_DATA=(SERVICE_NAME=MER)(CID=(PROGRAM=E:\\Navicat?for?Oracle avicat.exe)(HOST=DESKTOP-PSOJPQF)(USER=MER))(SERVER=dedicated)(INSTANCE_NAME=mer1))(ADDRESS=(PROTOCOL=tcp)(HOST=192.168.40.234)(PORT=1521))) Length: 293 主机与Scan进行四次挥手，断开tcp连接 主机向重定向节点，此处为节点1（vip）发起tcp连接 主机向节点1发送Connect消息，携带信息如下，此处的请求信息同scan返回响应一致（目的端口1521）： 1(DESCRIPTION=(CONNECT_DATA=(SERVICE_NAME=MER)(CID=(PROGRAM=E:\\Navicat?for?Oracle avicat.exe)(HOST=DESKTOP-PSOJPQF)(USER=MER))(SERVER=dedicated)(INSTANCE_NAME=mer1))(ADDRESS=(PROTOCOL=tcp)(HOST=192.168.40.234)(PORT=1521))) 节点1返回Accept 协商安全协议 协商数据类型 传输数据（用户名密码的验证是在这一部分做的）","tags":["Oracle RAC","Database"]},{"title":"Windows7 下安装 Oracle RAC11gR2","path":"/posts/324638463/","content":"本文介绍了如何在 Window7 环境下安装 Oracle RAC11gR2 的方法以及安装过程中遇到的一些问题的解决方法 资源 win64_11gR2_grid win64_11gR2_database_1of2 win64_11gR2_database_2of2 以上资源可在官网下载 安装文档以 Rac11gR2OnWindows.pdf 为准，由于文档上面有很多步骤比较省略，且更接近真实环境下的配置方案，因此找了 一篇安装步骤非常接近官方文档的博客1作为参考，同时结合其他博客进行安装 首先给出网络拓扑结构如下，后面遇到问题可以返回来作为参照 正式开始配置 配置步骤虚拟机准备 创建一个 Windows7 虚拟机（使用 VMware® Workstation 15 pro） 安装 jdk （版本应在 1.5 及以上） 为虚拟机添加一块网卡 克隆虚拟机 ✴︎节点名称配置修改主机名、统一用户名密码，分别更改两台服务器的主机名为 cluster1 和 cluster2，需要重启后生效，两台服务器统一使用 administrator 用户，并且保持密码一致。 修改主机名在“资源管理器-&gt;右键（计算机）-&gt;属性-&gt;更改设置（计算机名称、与和工作组）-&gt;更改“，保存后重启即可 用户必须使用 administrator 用户，需要在“资源管理器-&gt;右键（计算机）-&gt;管理-&gt;本地用户和组“中开启并设置密码 对所有节点执行上述操作，保证各节点 administrator 账户的密码相同，后续操作同样需要在 administrator 账户下进行，重启后注意切换用户 配置本地安全策略 运行 secpol.msc ,配置”安全设置-&gt;本地策略-&gt;安全选项-&gt;用户帐户控制:管理员批准模式中管理员的提升权限提示的行为”为”不提示，直接提升”。 确认”安全设置-&gt;本地策略-&gt;用户权限分配-&gt;管理审核和安全日志”中包括 Administrators 组。 运行 firewall.cpl ,关闭防火墙。 关闭防火墙需要在 “cmd-&gt;firewall.cp-&gt;高级设置-&gt;防火墙属性” 中确定防火墙状态为关闭 测试 net use在每个节点执行 net use \\\\remote node name\\C$ 提示命令成功完成则正确，否则不正确 &gt;&gt;执行net use失败&lt;&lt; 远程注册表连接测试运行 regedit ,选择 “文件-&gt;连接网络注册表-&gt;输入远程节点nodename” 出现注册表结构树,测试成功.(所有节点执行) ✴︎配置网络 在“网络和共享中心-&gt;更改适配器设置”中将网络适配器的名称改为”net0”和”net1” 按照拓扑图修改ip地址（net0为公网，net1为私网） 示例： cluster1： net0 -&gt; Public IP: 192.168.40.230&#x2F;24 net1 -&gt; Private IP: 10.0.0.95&#x2F;24 cluster2: net0 -&gt; Public IP: 192.168.40.232&#x2F;24 net1 -&gt; Private IP:10.0.0.97&#x2F;24 在“网络和共享中心-&gt;更改适配器设置-&gt;（按下alt）高级-&gt;高级设置”中修改连接顺序为net0-&gt;net1-&gt;… 修改hosts文件：位于C:\\Windows\\System32\\drivers\\etc 示例： #public 192.168.40.230 cluster1 192.168.40.232 cluster2 #private 10.0.0.95 cluster1-priv 10.0.0.97 cluster2-priv #vip 192.168.40.231 cluster1-vip 192.168.40.233 cluster2-vip #scan 192.168.40.234 cluster-scan 使用ping验证hosts文件修改是否成功 关闭DHCP媒体感知打开注册表定位到 HKLM\\SYSTEM\\CurrentControlSet\\Services\\Tcpip\\Parameters 子项,新建一个 DWORD 类型的键值 DisableDHCPMediaSense ,将值修改为 1. 重启后使用命令 netsh interface ipv4 show global 验证是否成功关闭. 关闭SNP Features12C:\\&gt;netsh int tcp set global chimney=disabledC:\\&gt;netsh int tcp set global rss=disabled 重启后使用命令 C:\\&gt;netsh interface ipv4 show global 验证是否成功关闭 停止MSDTC服务运行 services.msc ,将 Distributed Transaction Coordinator (MSDTC) 服务停止,并设为”手动”. 同步节点时间1、运行 Regedit 定位到 HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\W32Time\\Config 子项,将主键 MaxNegPhaseCorrection 数值修改为0,关闭注册表程序. 执行同步: C:\\&gt;W32tm /config /update (需要连接Internet) 2、或者：在 RAC2 服务器上运行命令：net time \\rac1 (查看 RAC1 的当前时间)然后在 RAC2 服务器上运行命令：net time \\rac1 /set /y (设置 RAC2 时间与 RAC1 同 步)（建议使用这种方式） 检查环境变量计算机 右键-&gt;属性-&gt;高级系统设置-&gt;环境变量,确认变量TMP和TEMP值相同. 配置 DEP 和 UAC1、分别配置两台服务器上的数据执行保护（DEP），选择‘仅为基本 windows程序和服务启用’，需要重启后才能生效（可配置完下一步后一起重启） 确认两台服务器上的 UAC，若需要配置则在重启后生效(默认不需要配置) 修改虚拟内存由于服务器的内存为 16G，按照 Oracle 的官方文档，虚拟内存至少为实际内 存的 2 倍，此处选择在本地磁盘一个较大的空白分区（E 盘）中划分了虚拟 内存，取值范围：32G~64G，即初始值：32768MB，最大值：65536MB ✴︎配置共享磁盘 这一步与使用的虚拟机及其版本有关，这里使用的是VMware® Workstation 15 pro，使用其他如VirtualBox请自行搜索配置方式 在虚拟机中添加2块磁盘分别用作OCR_VOTE（表决磁盘）以及DATA（数据磁盘）： 虚拟磁盘类型：SCSI(S) →创建新的虚拟磁盘（第一次创建，之后使用现有的虚拟磁盘） →磁盘大小（OCR：4G，DATA：16G，FRA：8G（如果有））、立即分配、存储为单个文件 在“虚拟机设置→选中硬盘→高级”中将新添加的虚拟设备节点按照顺序设为SCSI 1:x 在其他节点中添加相同数量的虚拟磁盘，并将设备节点一一对应设置 打开虚拟机文件（xxx.vmx）添加如下行（注意删除已存在的重复行）： 1234567891011disk.locking=&quot;FALSE&quot;disk.EnableUUID = &quot;TRUE&quot;diskLib.dataCacheMaxSize = &quot;0&quot;diskLib.dataCacheMaxReadAheadSize = &quot;0&quot;diskLib.dataCacheMinReadAheadSize = &quot;0&quot;diskLib.dataCachePageSize = &quot;0&quot;diskLib.maxUnsyncedWrites = &quot;0&quot;scsi1.shared=&quot;TRUE&quot;scsi1.virtualDev = &quot;lsilogic&quot;scsi1.sharedBus = &quot;VIRTUAL&quot;scsi1.present = &quot;TRUE&quot; 并在克隆的虚拟机vmx文件中额外添加如下行（取决于磁盘数量）： 123scsi1:0.mode = &quot;independent-persistent&quot;scsi1:1.mode = &quot;independent-persistent&quot;... 配置完成后重新打开“虚拟机设置”中可以看到克隆机显示磁盘为（独立） &gt;&gt;可能存在的问题&lt;&lt; 开机，在“计算机→（右键）管理→磁盘管理”中可以看到刚刚添加的磁盘，同时会提示需要对磁盘进行初始化（没有弹出提示的话可以手动全部选中，点击右键→初始化磁盘），并选择使用GPT初始化 为每一个节点执行上一步操作 为每一块磁盘选择“右键→新建简单卷→不分配驱动器号或驱动器路径→不要格式化这个卷“ 完成后应该显示磁盘为RAW设备 在另一台虚拟机上打开磁盘管理重新扫描磁盘即可 &gt;&gt;共享磁盘显示不正确&lt;&lt; 启用Automount (All Nodes)进行命令行窗口,运行: 12C:\\&gt; diskpartDISKPART&gt; AUTOMOUNT ENABLE 标记ASM磁盘在cluster1执行以下命令（cd至 grid安装目录\\asmtool） 1E:\\grid\\asmtool&gt;asmtoolg.exe 在弹出的图形化界面中进行以下操作： 选择“Add or change label“ →选中作为表决磁盘的虚拟硬盘 →修改下方标记为“OCR” →点击下一步 这一步只需在cluster1节点下操作，并且此时只标记OCR磁盘组，用于DATA组的硬盘暂不处理 ✴︎安装Grid Infrastructure (cluster1)执行预检查 D:\\11\\grid&gt;runcluvfy stage -pre crsinst -n rac1,rac2 -verbose 如果有报错,检查修改前面的设置直到预检查成功 在grid目录下执行setup.exe程序 选择“跳过软件更新” 选择‘为集群安装和配置 集群的网络基础结构’ 选择‘高级安装’ 选择安装语言 填写集群名称和 SCAN 名称，必须填写 hosts 文件中 SCAN IP 对应的主机名,确认去掉“配置GNS”复选框。 示例： 集群名称：mer-cluster scan名称：cluster-scan scan端口：1521 添加其他节点（应当显示cluster1和cluster2） 更改网络接口类型，默认选项如果不对，需要手动更改 此处选择‘自动存储管理（ASM） 选择相应的OCR磁盘组，并输入磁盘组名称OCR 示例： 磁盘组名：OCR 冗余：外部 添加磁盘：候选磁盘（选择标记为OCR的磁盘路径） 添加 ASM （ASMSnmp）口令（topsec） 由于设置的密码不符合标准，需要确认来强制使用 此处选择‘不使用 IPMI 选择软件相关目录 示例： Oracle基目录：C:\\app\\Administrator 软件位置：C:\\app\\11.2.0\\grid 先决条件检查，如果检查通过，则出现概要，如果提示失败，会有相应的提示，请根据提示 检查之前步骤中的设置是否存在问题 开始安装后，会在”网格基础结构配置“处停顿较长时间，此处也是 grid 安装成功与否的关键所在，如果此处出现问题，则需要 卸 载 grid，并检查之前所有的设置，然后再次尝试安装，直到显示成功为止。 安装成功完成 检查ora.asm资源运行状态grid 安装完成后，如果安装成功，可在 dos 环境下通过 crs_stat –t –v 命令或者 crsctl status res -t 查看集 群启动了哪 些服务 ✴︎安装RDBMS（cluster1）执行预检查 D:\\grid&gt;runcluvfy stage -pre dbinst -n rac1,rac2 -verbose 开始安装 执行database目录下setup.exe程序 确认跳过‘指定电子邮件地址 选择“跳过软件更新” 选择‘仅安装数据库软件 查看节点名称是否正确 选择安装语言 选择安装企业版，并且勾选所有组件 选择安装路径 示例： Oracle基目录：C:\\app\\Administrator 软件位置：C:\\app\\Administrator\\product\\11.2.0\\db_home1 先决条件检查 安装概要 安装到此处时，执行远程安装 RAC2 节点的数据库，此时等待时间会很长， 请耐心等待 ✴︎安装完成后需要根据提示在cluster2节点上执行操作（注意别随手关闭提示） C:\\app\\Administrator\\product\\11.2.0\\dbhome_1\\BIN\\selecthome.bat 执行过程中出现的计数器尚未安装可以忽略 &gt;&gt;安装RDBMS失败（不要关闭提示窗口）&lt;&lt; 使用ASMCA创建ASM磁盘组 在命令行下执行asmca 在磁盘组选项卡中点击新建 单击在磁盘上加载标记 参考标记OCR的方式标记DATA磁盘 标记完成后填写磁盘组为DATA，冗余为外部，选择标记的磁盘，点击确定 创建磁盘组 按照相同的方法标记和创建 FRA 磁盘组（有需要的话） 创建完成后退出. ✴︎创建数据库执行预检查 D:\\grid&gt;runcluvfy stage -pre dbcfg -n all -d D:\\app\\Administrator\\product\\11.2.0\\db1 创建数据库 D:\\&gt;dbca &gt;&gt;找不到dbca命令&lt;&lt; 选择RAC数据库 选择创建数据库 一般用途或事务处理 配置数据库标示 示例： 配置类型：管理员管理的 全局数据库名：mer SID前缀：mer 单击全选 配置管理选项 示例： 勾选配置Enterprise Manager 选择配置Database Control进行本地管理 不勾选预警通知与每日备份 对所有账户使用同一管理口令 使用Oracle-Managed Files，选择数据库区为刚刚标记的DATA磁盘组 输入之前设置的ASM口令 取消勾选快速恢复区 配置字符集→从字符集列表中选择→”ZHS16GBK - GBK16位简体中文“ 数据库存储 创建数据库 数据库概要 等待创建完成 &gt;&gt;数据库创建失败&lt;&lt; 数据库创建完成中可能弹出提示如下: 此时按照提示在相应节点上（此处为cluster2）执行命令： 首先确定dbconsole状态： ​ C:\\&gt;emctl status dbconsole 若显示：Environment variable ORACLE_UNQNAME not defined. Please define it. 则设置环境变量ORACLE_UNQNAME: ​ C:\\&gt;set ORACLE_UNQNAME&#x3D;mer 若提示其他（如sid）未定义，则也设置相应的值 之后执行提示的第二步命令： ​ C:\\…\\db_home\\bin\\emctl.bat start dbconsole 执行完毕后回到cluster1节点点击确定提示集群数据库创建完成，并返回管理url 点击口令管理，解锁scott账户，设定密码 至此集群安装完成，在web端输入db control url即可访问管理登录界面，使用 用户名：SYSTEM 密码：topsec（之前设置的统一管理密码） 即可登录到集群管理界面 &gt;&gt;登录失败&lt;&lt; 可能遇到的问题及解决方法1、执行net use失败 错误号5，拒绝访问：很可能你使用的用户不是管理员权限的，先提升权限；错误号51，Windows无法找到网络路径：网络有问题；错误号53，找不到网络路径：ip地址错误；目标未开机；目标lanmanserver服务未启动；目标有防火墙（端口过滤）；错误号67，找不到网络名：你的lanmanworkstation服务未启动或者目标删除了ipc$；错误号1219，提供的凭据与已存在的凭据集冲突：你已经和对方建立了一个ipc$，请删除再连；错误号1326，未知的用户名或错误密码：原因很明显了；错误号1792，试图登录，但是网络登录服务没有启动：目标NetLogon服务未启动；错误号2242，此用户的密码已经过期：目标有帐号策略，强制定期要求更改密码； 确定当前登录用户为administrator 确定已为administrator用户设定密码 执行net share确定包含C$ 在”Windows+R”中执行secpol.msc，确定“安全设置→本地策略→安全选项”中“网络访问：本地账户的共享和安全模型”设置为“经典：对本地用户…“ 2、vmx文件中可能存在的问题 在多数的博客中，设置disk.EnableUUID与scsi1.virtualDev的值为如下 12disk.EnableUUID = &quot;TRUE&quot;scsi1.virtualDev = &quot;lsilogic&quot; 但在少数博客中将这个值设置（或保持原来的值）为 12disk.EnableUUID = &quot;FALSE&quot;scsi1.virtualDev = &quot;lsisas1068&quot; 经测试这两种值似乎不影响共享磁盘的效果，但这里仍旧推荐使用第一种的配置值 3、共享磁盘显示不正确 检查是否所有节点都显示磁盘以联机，尝试重新配置 4、安装RDBMS失败 预检查成功，但安装时提示对于节点磁盘空间的先决检查失败 可以忽略该问题，对后续安装过程不造成影响 安装中途cluster2死机或重启导致安装中断 暂时不要关闭提示窗口，重启cluster2并切换至administrator账户，等待至集群服务启动后（各vip、scan-ip可以ping通）返回cluster1点击确定可以继续安装 所选安装与指定 Oracle 主目录中已安装的软件冲突 的问题 正确的做法是卸载RDBMS，清除相关文件、注册表以及服务并重新安装，但是没有找到相关的文档，删除所有Oracle项又会对以安装的grid项造成影响，因此此处选择修改数据库软件安装路径，将dbhome_1修改为dbhome_2，修改后可能会出现“无法启动&#x2F;关闭&#x2F;找到指定路径 OracleMTSRecoveryService”，解决方法同下 OracleMTSRecoveryService 无法启动&#x2F;关闭&#x2F;找到指定路径 首先打开Win+R，“（输入）services.msc→（找到）OracleMTSRecoveryService “，若该服务不存在则不适用于下述解决方法，初次安装时提示无法启动不适用 确定该服务存在并确定其启动类型为自动后，打开Win+R，“（输入）regedit→（找到）HKEY_LOCAL_MACHINE→SYSTEM→CurrentControlSet→services→OracleMTSRecoveryService”，修改ImagePath的值为新的安装路径（dbhome_2），并对以下位置进行同样的修改： HKEY_LOCAL_MACHINE→SYSTEM→CurrentControlSet→services→OracleMTSRecoveryService HKEY_LOCAL_MACHINE→SYSTEM→ControlSet001→services→OracleMTSRecoveryService HKEY_LOCAL_MACHINE→SYSTEM→ControlSet002→services→OracleMTSRecoveryService 其他节点的对应位置 修改完成后点击确定关闭提示窗口继续安装 若是初次安装时提示无法启动OracleMTSRecoveryService服务，则在cluster2节点打开Win+R，“（输入）services.msc→（找到）OracleRemExecService “，若ImagePath中的路径在Temp后有两个\\，则删除后刷新注册表（重启explorer）,返回cluster1节点点击重试即可 5、数据库创建失败 ‘dbca’ 不是内部或外部命令，也不是可运行的程序或批处理文件： 重启cmd，若仍找不到命令请确定RDBMS安装正确 配置em失败，提示如下： 该问题未知明确的解决流程，首先尝试重新运行dbca，若再次失败则按照提示运行使用emca脚本，命令为 emca -config dbcontrol db -repos create 按照提示输入 确定后可能提示实例不存在或其他错误，返回重新执行dbca，若依旧失败需自行寻找解决办法 已明确的： ​ 不需要配置监听程序，即使查看监听程序不存在 ​ 此时emca执行失败是正常的，create或是recreate ​ 大概率在emca失败后重新执行dbca可以解决该问题（可能的原因，emca执行过程中创建了监听器等dbca所需的环境） 6、登录失败 若确定用户名&#x2F;密码正确，尝试重启cluster 重启后执行crsctl check crs，确定连接正常后再行尝试登录 若依旧失败尝试使用以下命令（administrator账户下） 12345678（1）以sysdba身份登录，不需要提供用户名和密码。 sqlplus /as sysdba ; （2）为用户解锁。alter user system account unlock;（3）重新设定密码。alter user system identified by system123456; 更多任何情况下提示如下问题时： CRS-4639：无法连接 Oracle 高可用性服务，ohasd.bin 未运行或 ohasd.bin 虽在运行但无 init.ohasd 或其他进程 CRS-4530：联系集群同步服务守护进程时出现通信故障，ocssd.bin 未运行 CRS-4535：无法与集群就绪服务通信，crsd.bin 未运行 请检查crsctl check crs是否返回对应服务正常，若刚重启节点，请等待一段时间，否则请重启节点再试 安装完成后的检查项： 123456789crs_stat -t -vsrvctl status listener -n cluster1srvctl status listener -n cluster2srvctl config database -d mersrvctl status database -d mer 参考 RAC11gR2OnWindows.pdf Oracle 11G RAC For Windows 2008 R2部署手册（亲测，成功实施多次） Oracle11gR2 RAC for Windows安装上篇 在安装oracle 11g时，出现执行先决条件失败的情况如下 oracle下system用户解锁和改密码 EM Express不起作用的故障排除 (Doc ID 1604062.1) 监听程序未启动或数据库服务未注册到该监听程序解决方法 net use访问远程电脑 ASM磁盘、目录的管理 安装oracle 11g时出现启动服务出现错误，找不到OracleMTSRecoveryService 以sysdba身份登录oracle报ORA-1031权限不足错误之完美分析 Oracle中sqlplus登录报错SP2-0667和SP2-0750探究","tags":["Oracle RAC","Database"]},{"title":"Nginx 多阶段 Http 请求处理流程","path":"/posts/649054200/","content":"Nginx 通过将各个阶段的所有模块按序的组织成一条执行链，以流水线的形式依次进行处理。 执行链及流程nginx执行链的定义如下： 12345typedef struct &#123; ngx_http_phase_handler_t *handlers; /*执行链*/ ngx_uint_t server_rewrite_index; ngx_uint_t location_rewrite_index;&#125; ngx_http_phase_engine_t; 执行链节点的数据结构定义如下： 12345struct ngx_http_phase_handler_s &#123; ngx_http_phase_handler_pt checker; ngx_http_handler_pt handler; ngx_uint_t next;&#125;; 对于执行链节点，相同阶段具有相同的checker函数，handler中则保存的是挂载至该阶段的模块处理函数，一般在checker函数中会执行当前节点的handler函数 执行链的执行流程是按照执行链顺序向前执行，但某个阶段需要回跳或跳过之后的某些执行阶段，next字段保存的就是跳跃的目的索引。 各个阶段如下： NGX_HTTP_POST_READ_PHASE 接受完请求头之后的第一个阶段，位于uri重写之前，默认情况下该阶段被跳过; nginx源码中仅有realip模块在POST_READ阶段对客户端ip进行了替换，且该模块并未被默认编译进nginx POST_HEAD阶段的checker函数仅调用对应的handler函数，随后对返回值进行处理，当handler返回NGX_OK时进入下一阶段 NGX_HTTP_SERVER_REWRITE_PHASE server级别的uri重写阶段，执行于server块内，location块外的重写指令。 nginx的rewrite模块在此阶段提供url重写指令rewrite和变量指令set，以及逻辑控制指令if、break和return以供用户完成一些简单的需求而不必注册handler 该阶段的请求未被匹配至具体的location中 SERVER_REWRITE阶段的checker函数同样调用handler函数，但处理方式稍有不同，当handler返回NGX_DECLINED时进入下一阶段 NGX_HTTP_FIND_CONFIG_PHASE 这一阶段根据重写过的uri查找对应的location，可能被执行多次 通过ngx_http_core_find_location函数完成对r-&gt;loc_conf的设置，然后调用ngx_http_update_location_config更新请求相关配置 NGX_HTTP_REWRITE_PHASE location级别的重写阶段，该阶段执行location基本的重写指令，可能被执行多次。 与SERVER_REWRITE的逻辑基本相同，使用相同的handler函数，只是执行的时机不同。 NGX_HTTP_POST_REWRITE_PHASE 该阶段不能挂载handler，仅用于检查REWRITE阶段是否进行uri重写，若发生重写则返回再次执行REWRITE阶段，默认限制的重写次数为10次 介入该阶段的模块同样是rewrite模块 NGX_HTTP_PREACCESS_PHASE 访问权限控制的前一阶段，进入这一阶段时请求的loc_conf配置已经确定，在此阶段一般用于进行资源配置，限制连接数或请求速率。 ngx_http_limit_conn_module和ngx_http_limit_req_module等模块会在该阶段注册handler NGX_HTTP_ACCESS_PHASE 访问权限控制阶段,例如基于ip黑白名单的权限控制，或者基于用户名密码的权限控制。 默认情况下nginx 的 ngx_http_access_module和ngx_http_auth_basic_module模块分别会在该阶段注册一个handler 需要注意的是在此阶段需要满足所有的handler验证，即所有handler返回NGX_OK时，该阶段的checker函数才会进入下一阶段。 NGX_HTTP_POST_ACCESS_PHASE 此阶段仅根据ACCESS阶段的执行结果进行相应处理，不能挂载handler。 当ACCESS阶段返回NGX_HTTP_FORBIDDEN或NGX_HTTP_UNAUTHORIZED时，会在此阶段结束请求 NGX_HTTP_TRY_FILES_PHASE 处理try_files指令，如果没有配置try_files指令，该阶段会被跳过。 try_file指令用于检查指定的一个或多个文件或目录是否存在，若存在则执行之后的阶段，否则返回指定的lcoation或指定的返回码 此阶段不能挂载handler NGX_HTTP_CONTENT_PHASE 内容生成阶段，该阶段产生响应，并发送至客户端 在CONTENT阶段中，checker函数首先检查是否设置了content_handler，这里的content_handler不同于挂载在执行链上的handler，是每个location都可以独立拥有的，若存在则nginx在CONTENT阶段会直接执行content_handler，而不会再执行本阶段的handler。 在执行content_handler之前，nginx会将请求的write_event设置为ngx_http_request_empty_handler，这表示如果模块所设置的content_handler涉及到IO操作，则需要合理的设置读写 事件handler 同样的nginx将r-&gt;content_handler(r)的返回值直接传入ngx_http_finalize_request中，因此如果content_handler并未完成整个请求的处理，就需要设置合适的返回值并将请求的引用 计数加1以防请求被释放 如果没有注册content_handler，则与之前的阶段类似，checker会调用handler函数，而由于CONTENT是ngx_http_core_run_phases的最后一个阶段，因此若handler未返回NGX_DECLINED，checker将会结束请求并返回NGX_OK，否则将会返回NGX_FORBIDDEN或NGX_HTTP_NOT_FOUND NGX_HTTP_LOG_PHASE 日志记录阶段，该阶段记录访问日志，进入该阶段标明该请求的响应已经发送到系统的发送缓冲区中。 LOG阶段的执行位于ngx_http_free_request中，在这个阶段中会遍历LOG阶段的所有handler并执行 HTTP proxy模块HttpProxy模块用于将请求导向其他服务 Nginx与客户端使用HTTP&#x2F;1.1通信，而在后台服务使用HTTP&#x2F;1.0通信 在Nginx中, HTTP proxy模块介入于HTTP处理流程的CONTENT阶段，proxy模块通过”proxy_pass”配置的ngx_http_proxy_handler（位于ngx_http_proxy_module.c)中 rc = ngx_http_read_client_request_body(r, ngx_http_upstream_init); 设置处理请求的post_hadnler为ngx_http_upstream_init 而在 ngx_http_read_client_request_body 中 12345if (r != r-&gt;main || r-&gt;request_body || r-&gt;discard_body) &#123; r-&gt;request_body_no_buffering = 0; post_handler(r); return NGX_OK; &#125; 将请求交由upstream处理 部分proxy模块配置 proxy_pass：用于指定方向代理服务器的服务器池proxy_set_header:用于添加一些请求头，传递至代理服务器。 例如 proxy_set_header Host $http_host 用于区分后端主机 proxy_set_header X-Real_IP $remote_addr 以便后端获得客户端的真实IPproxy_body_buffer_size:指定缓冲区大小proxy_connect_timeout:指定与后端连接的超时时间proxy_send_timeout:指定后端服务器的数据回传超时时间 upstream模块Upstream模块与Handler相似，区别在于upstream模块不产生内容,而是通过请求后端服务器得到内容，在使用中upstream模块只需开发若干回调函数，完成构造请求和解析响应等工作 upstream模块的处理流程 1.创建upstream数据结构2.设置模块的tag和schema,其中schema用于日志，tag用于buf_chain管理3.设置upstream的后端服务器列表4.设置upstream回调函数5.创建并设置upstream环境6.完成初始化并进行收尾工作 此upstream的实际行为主要有两点，同上游主机建立连接并获取资源，以及将从上游主机中获取到的资源转发至下游客户端 而在同上游主机建立连接之前，nginx首先需要决定应当同 upstream 配置的后端主机列表中的哪一个建立连接 负载均衡模块用于从 upstream 指令所定义的后端主机列表中选取一台主机，以便进行之后建立连接以及获取相应的资源。 nginx内置的负载均衡模块主要有两种，默认为轮询模式，另一种则是ip_hash模式 ip_hash的主要逻辑位于 ngx_http_upstream_ip_hash 中，在这里这个函数主要做了两件事，对uscf-&gt;flags进行设置，以及设置init_upstream的回调函数为 ngx_http_upstream_init_ip_hash 在 ngx_http_upstream_init_ip_hash 中upstream将peer.init 设置为 ngx_http_upstream_init_ip_hash_peer（这个函数将会在 upstream 初始化请求时被调用） 通过调用peer.init，nginx会为每一个请求构造一张包含所有可用的upstream服务器的表，用于负载均衡计算以及提供当服务器宕机时的储备 同样在 ip_hash_peer 中，将upstream-&gt;peer.get的回调函数设置为 ngx_http_upstream_get_ip_hash_peer该函数负责从服务器表中取出某个服务器，通过get函数的返回值，nginx可以了解是否存在可用连接以及连接是否被建立 可能的返回值如下： 123NGX_DONE: 得到连接地址，且连接已被建立NGX_OK: 得到连接地址，但连接并未建立NGX_BUSY: 所有连接均不可用 得到并建立一个连接之后,upstream就可以尝试向此连接中发送请求头和请求体了，upstream使用 ngx_http_upstream_send_request 向后端发送请求 而在得到来自上游服务器的响应之后，upstream通过 ngx_http_upstream_process_header 将来自上游服务器的响应内容进行处理，并通过 ngx_http_upstream_send_response 返回至客户端 最后，upstream通过 peer.free 和 ngx_http_upstream_finalize 释放资源和连接 以上就是 Nginx 多阶段处理 Http 请求的全部过程","tags":["Nginx"]},{"title":"Manacher's Algorithm（求最长回文子串）","path":"/posts/4278205636/","content":"本文讲述 Manacher 算法（马拉车算法）的原理以及如何利用 Manacher 算法获取最长回文子串。 给定一个字符串s，找到这个字符串的最长回文子串 Example 1: 123Input: &quot;babad&quot;Output: &quot;bab&quot;Note: &quot;aba&quot; is also a valid answer. Example 2: 12Input: &quot;cbbd&quot;Output: &quot;bb&quot; 要找到一个字符串中的最长回文子串有以下几种方法： 暴力搜索，算法复杂度O(N3) 动态规划，算法复杂度O(N2) Manacher算法，算法复杂度O(N) 而本文的主角就是Manacher’s algorithm，一起来看看它的实现 核心思想Manacher算法的核心思想在于搜索以每个字符为中心的回文子串长度，并利用先前搜索的结果加快后续回文子串长度的计算 其中从中心向两边搜索是Manacher和动态规划方法相比暴力搜索降低复杂度的关键，而利用先前的搜索结果加快后续回文子串计算则是Manacher算法能够以不可思议的O(N)复杂度完成回文串查找的关键 过程以Manacher算法的一个中间过程为例 s：给定的字符串 P：手动维护，P[i]保存以i位置为中心的最长子串长度 Center：当前最近的回文子串中心 Left：最近回文子串的左边界 RIght：最近回文子串的右边界 i：当前位置 要计算i位置的回文子串长度P[i]，根据Manacher算法的核心思想，我们需要以i为中心，向两边扩张计算回文子串的长度，而在这之前，我们可以通过一些方式获得P[i]的最小值以减少计算量，一共有以下两种情况： 当前位置i位于右边界R之前 当前位置i位于右边界R之后 首先讨论第一种i &lt; R的情况，也就是如上图所示的情况 在这种情况下，由于i在以Center为中心的回文子串内，因此我们可以通过下图中与i相对的i_mirror来加速对P[i]的运算 i_mirror &#x3D; 2 * Center - i 由于串C是回文的，因此以i为中心的回文子串最少应该与以i_mirror为中心的回文子串长度相当（中心对称），如下图： ![截屏2020-07-25 08.15.34](&#x2F;images&#x2F;Manacher-s-Algorithm&#x2F;截屏2020-07-25 08.15.34.png) 此时P[i] = P[i_mirror] 那么这是唯一的情况么，答案是否定的，在上图中由于串i_mirror完全包含在串C当中，因此我们可以认为串i与串i_mirror是中心对称的，可以复用其长度，然而若回文串i_mirror与串C只有部分重合呢？考虑如下图所示的情况 在对给定的字符串做了一些小小的改动后（如绿色字体所示），我们得到了一组新的回文串C，此时我们可以看到，位置i相对于Center的镜像位置i_mirror，其最长回文串不是包含在串C中的（仅有部分重叠），那么这种情况下P[i]的值又该是多少呢? 答案是R-i 这里实际上借助了中心对称的特性，使用R-i计算串i_mirror与串C重叠的部分 因此综合之前的情况，在第一种情况下，P[i]的最小值为 min(R-i, P[i_mirror]) 其中i_mirror = 2 * C - i 接下来讨论第二种情况，当i &gt; R时，P[i]的值应该是多少 答案应该是显而易见的，当i &gt; R时，我们没有任何可以参考的信息（这和计算P[0]的情况是相当的），因此这时P[i] = 0，我们需要从0开始逐步向两边扩张 综合上述两种情况，我们就得到了任意位置的P[i]初始值为 R &gt; i ? min( R-i, P[i_mirror]) : 0 在获得了初始值之后，我们就可以以此为起点，不断向两边扩张了，以代码形式我们的行为是这样的： 12345//直到以i为中心的s[left] != s[right]为止while( s[ i-1-P[i] ] == s[ i+1+P[i] ]) &#123; //回文串i扩张 P[i] ++;&#125; 在这之后若新的回文串i的右边界超过了原先串C的右边界，那么我们需要更新串C为当前的串i，原因是计算后续位置的回文串时在串C内的部分至少在串i内也是回文的 按照此步骤不断进行，最后我们就可以得到一个填充完毕的P[i]如下： 其中最大值6就是我们想要得到的最长回文子串长度，返回该部分子串即可 ReferencesManacher’s Algorithm 最长回文子串——Manacher 算法","tags":["C++","Algorithm"]},{"path":"/google2ffb9a159ec4029a.html","content":"google-site-verification: google2ffb9a159ec4029a.html"}]